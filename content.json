{"meta":{"title":"DexBlog","subtitle":"罗德新的网络笔记","description":"","author":"Dexin Luo","url":"http://blog.luodexin.com","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2020-11-07T10:41:10.601Z","updated":"2020-11-07T10:41:10.601Z","comments":false,"path":"/404.html","permalink":"http://blog.luodexin.com/404.html","excerpt":"","text":""},{"title":"关于","date":"2017-12-06T15:22:50.000Z","updated":"2020-11-09T05:35:08.167Z","comments":false,"path":"about/index.html","permalink":"http://blog.luodexin.com/about/index.html","excerpt":"","text":"TALK IS CHEAP, SHOW ME THE CODE."},{"title":"书单","date":"2020-11-06T16:51:28.611Z","updated":"2020-11-06T16:51:28.611Z","comments":false,"path":"books/index.html","permalink":"http://blog.luodexin.com/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2020-11-06T16:49:32.720Z","updated":"2020-11-06T16:49:32.718Z","comments":false,"path":"categories/index.html","permalink":"http://blog.luodexin.com/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2020-11-07T11:16:25.043Z","updated":"2020-11-07T11:16:25.043Z","comments":false,"path":"links/index.html","permalink":"http://blog.luodexin.com/links/index.html","excerpt":"","text":""},{"title":"标签","date":"2020-11-06T16:50:41.804Z","updated":"2020-11-06T16:50:41.803Z","comments":false,"path":"tags/index.html","permalink":"http://blog.luodexin.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Kafka常见面试题","slug":"mq/Kafka/03.Kafka面试题","date":"2020-11-01T15:47:21.000Z","updated":"2020-11-07T11:38:57.285Z","comments":false,"path":"posts/fe06768f.html","link":"","permalink":"http://blog.luodexin.com/posts/fe06768f.html","excerpt":"","text":"1. 什么是kafkaKafka是分布式发布-订阅消息系统，它最初是由LinkedIn公司开发的，之后成为Apache项目的一部分，Kafka是一个分布式，可划分的，冗余备份的持久性的日志服务系统，它主要用于处理流式数据。 2. 为什么要使用 kafka，为什么要使用消息队列缓冲和削峰：上游数据时有突发流量，下游可能扛不住，或者下游没有足够多的机器来保证冗余，kafka在中间可以起到一个缓冲的作用，把消息暂存在kafka中，下游服务就可以按照自己的节奏进行慢慢处理。 解耦和扩展性：项目开始的时候，并不能确定具体需求。消息队列可以作为一个接口层，解耦重要的业务流程。只需要遵守约定，针对数据编程即可获取扩展能力。 冗余：可以采用一对多的方式，一个生产者发布消息，可以被多个订阅topic的服务消费到，供多个毫无关联的业务使用。 健壮性：消息队列可以堆积请求，所以消费端业务即使短时间死掉，也不会影响主要业务的正常进行。 异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 3. Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么ISR: In-Sync Replicas 副本同步队列AR: Assigned Replicas 所有副本ISR是由leader维护，follower从leader同步数据有一些延迟（包括延迟时间replica.lag.time.max.ms和延迟条数replica.lag.max.messages两个维度, 当前最新的版本0.10.x中只支持replica.lag.time.max.ms这个维度），任意一个超过阈值都会把follower剔除出ISR, 存入OSR（Outof-Sync Replicas）列表，新加入的follower也会先存放在OSR中。AR=ISR+OSR。 4. kafka中的broker 是干什么的broker 是消息的代理，Producers往Brokers里面的指定Topic中写消息，Consumers从Brokers里面拉取指定Topic的消息，然后进行业务处理，broker在中间起到一个代理保存消息的中转站。 5. kafka中的 zookeeper 起到什么作用，可以不用zookeeper么zookeeper 是一个分布式的协调组件，早期版本的kafka用zk做meta信息存储，consumer的消费状态，group的管理以及 offset的值。考虑到zk本身的一些因素以及整个架构较大概率存在单点问题，新版本中逐渐弱化了zookeeper的作用。新的consumer使用了kafka内部的group coordination协议，也减少了对zookeeper的依赖， 但是broker依然依赖于ZK，zookeeper 在kafka中还用来选举 leader 和 检测broker是否存活等等。 6. kafka follower如何与leader同步数据Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。完全同步复制要求All Alive Follower都复制完，这条消息才会被认为commit，这种复制方式极大的影响了吞吐率。而异步复制方式下，Follower异步的从Leader复制数据，数据只要被Leader写入log就被认为已经commit，这种情况下，如果leader挂掉，会丢失数据。Kafka使用ISR的方式很好的均衡了确保数据不丢失以及吞吐率。Follower可以批量的从Leader复制数据，而且Leader充分利用磁盘顺序读以及send file(zero copy)机制，这样极大的提高复制性能，内部批量写磁盘，大幅减少了Follower与Leader的消息量差。 7. 什么情况下一个 broker 会从 isr中踢出去leader会维护一个与其基本保持同步的Replica列表，该列表称为ISR(in-sync Replica)，每个Partition都会有一个ISR，而且是由leader动态维护 ，如果一个follower比一个leader落后太多，或者超过一定时间未发起数据复制请求，则leader将其重ISR中移除 。 8. kafka 为什么那么快Cache Filesystem Cache PageCache 缓存 顺序写 由于现代的操作系统提供了预读和写技术，磁盘的顺序写大多数情况下比随机写内存还要快。 Zero-copy 零拷贝技术 减少拷贝次数 Batching of Messages 批量量处理。合并小的请求，然后以流的方式进行交互，直顶网络上限。 Pull 拉模式 使用拉模式进行消息的获取消费，与消费端处理能力相符。 9. kafka producer如何优化打入速度增加线程 提高 batch.size 增加更多 producer 实例 增加 partition 数 设置 acks=-1 时，如果延迟增大：可以增大 num.replica.fetchers（follower 同步数据的线程数）来调解； 跨数据中心的传输：增加 socket 缓冲区设置以及 OS tcp 缓冲区设置。 10. kafka producer 打数据，ack 为 0， 1， -1 的时候代表啥， 设置 -1 的时候，什么情况下，leader 会认为一条消息 commit了1（默认） 数据发送到Kafka后，经过leader成功接收消息的的确认，就算是发送成功了。在这种情况下，如果leader宕机了，则会丢失数据。0 生产者将数据发送出去就不管了，不去等待任何返回。这种情况下数据传输效率最高，但是数据可靠性确是最低的。-1 producer需要等待ISR中的所有follower都确认接收到数据后才算一次发送完成，可靠性最高。当ISR中所有Replica都向Leader发送ACK时，leader才commit，这时候producer才能认为一个请求中的消息都commit了。 11. kafka unclean 配置代表啥，会对 spark streaming 消费有什么影响unclean.leader.election.enable 为true的话，意味着非ISR集合的broker 也可以参与选举，这样有可能就会丢数据，spark streaming在消费过程中拿到的 end offset 会突然变小，导致 spark streaming job挂掉。如果unclean.leader.election.enable参数设置为true，就有可能发生数据丢失和数据不一致的情况，Kafka的可靠性就会降低；而如果unclean.leader.election.enable参数设置为false，Kafka的可用性就会降低。 12. 如果leader crash时，ISR为空怎么办kafka在Broker端提供了一个配置参数：unclean.leader.election,这个参数有两个值：true（默认）：允许不同步副本成为leader，由于不同步副本的消息较为滞后，此时成为leader，可能会出现消息不一致的情况。false：不允许不同步副本成为leader，此时如果发生ISR列表为空，会一直等待旧leader恢复，降低了可用性。 13. kafka的message格式是什么样的一个Kafka的Message由一个固定长度的header和一个变长的消息体body组成header部分由一个字节的magic(文件格式)和四个字节的CRC32(用于判断body消息体是否正常)构成。当magic的值为1的时候，会在magic和crc32之间多一个字节的数据：attributes(保存一些相关属性，比如是否压缩、压缩格式等等);如果magic的值为0，那么不存在attributes属性body是由N个字节构成的一个消息体，包含了具体的key/value消息 CRC ： 校验码Magic ：版本号：0，1，2，对应目前的三个版本Attribute ： 属性字段，目前使用后3位存储压缩类型Key len ：key长度Key ：key值Value len：value长度Value ：具体消息内容 14. kafka中consumer group 是什么概念同样是逻辑上的概念，是Kafka实现单播和广播两种消息模型的手段。同一个topic的数据，会广播给不同的group；同一个group中的worker，只有一个worker能拿到这个数据。换句话说，对于同一个topic，每个group都可以拿到同样的所有数据，但是数据进入group后只能被其中的一个worker消费。group内的worker可以使用多线程或多进程来实现，也可以将进程分散在多台机器上，worker的数量通常不超过partition的数量，且二者最好保持整数倍关系，因为Kafka在设计时假定了一个partition只能被一个worker消费（同一group内）。 15. Kafka中的消息是否会丢失？要确定Kafka的消息是否丢失或重复，从两个方面分析入手：消息发送和消息消费。 1. 生产者端消息丢失Kafka消息发送方式有两种：同步（sync）和异步（async），默认是同步方式，可通过producer.type属性进行配置。 同步: 这个生产者写一条消息的时候，它就立马发送到某个分区去。follower还需要从leader拉取消息到本地，follower再向leader发送确认，leader再向客户端发送确认。由于这一套流程之后，客户端才能得到确认，所以很慢。 异步: 这个生产者写一条消息的时候，先是写到某个缓冲区，这个缓冲区里的数据还没写到broker集群里的某个分区的时候，它就返回到client去了。虽然效率快，但是不能保证消息一定被发送出去了。 Kafka消息生产确认方式有三种，通过配置request.required.acks属性来确认消息的生产： 0: 表示不进行消息接收是否成功的确认； 1: 表示当Leader接收成功时确认； -1: 表示Leader和Follower都接收成功时确认； 生产端有6种消息生产的情况，下面分情况来分析消息丢失的场景：（1）acks=0，不和Kafka集群进行消息接收确认，则当网络异常、缓冲区满了等情况时，消息可能丢失；（2）acks=1、同步模式下，只有Leader确认接收成功后但挂掉了，副本没有同步，数据可能丢失； 2. 消费者端消息丢失：consumer端丢失消息的情形比较简单：消息消费者从集群中把消息取出来、并提交了新的消息offset值后，还没来得及消费就挂掉了，那么下次再消费时之前没消费成功的消息就“诡异”的消失了； 消息丢失解决办法： 生产端：同步模式下，确认机制设置为-1，即让消息写入Leader和Follower之后再确认消息发送成功；异步模式下，为防止缓冲区满，可以在配置文件设置不限制阻塞超时时间，当缓冲区满时让生产者一直处于阻塞状态 消费端enable.auto.commit=false 关闭自动提交位移在消息被完整处理之后再手动提交位移 16. 如何保证消息不会被重复消费？消息重复消费的原因比如 RabbitMQ、RocketMQ、Kafka，都有可能会出现消息重复消费的问题，正常。因为这问题通常不是 MQ 自己保证的，是由我们开发来保证的。Kafka 实际上有个 offset 的概念，就是每个消息写进去，都有一个 offset，代表消息的序号，然后 consumer 消费了数据之后，每隔一段时间（定时定期），会把自己消费过的消息的 offset 提交一下，表示“我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的 offset 来继续消费吧”。 但是凡事总有意外，比如我们之前生产经常遇到的，就是你有时候重启系统，看你怎么重启了，如果碰到点着急的，直接 kill 进程了，再重启。这会导致 consumer 有些消息处理了，但是没来得及提交 offset，尴尬了。重启之后，少数消息会再次消费一次。 如果确保消息不会重复消费其实重复消费不可怕，可怕的是你没考虑到重复消费之后，怎么保证幂等性。幂等性，通俗点说，就一个数据，或者一个请求，给你重复来多次，你得确保对应的数据是不会改变的，不能出错。一条数据重复出现两次，数据库里就只有一条数据，这就保证了系统的幂等性。 其实还是得结合业务来思考，我这里给几个思路：比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。","categories":[{"name":"MQ - Kafka","slug":"MQ-Kafka","permalink":"http://blog.luodexin.com/categories/MQ-Kafka/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://blog.luodexin.com/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"http://blog.luodexin.com/tags/Kafka/"},{"name":"面试","slug":"面试","permalink":"http://blog.luodexin.com/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"Spring Cloud 微服务网关--Zuul","slug":"spring-cloud/zuul/04.Zuul","date":"2019-06-07T12:17:59.000Z","updated":"2020-11-07T11:38:57.287Z","comments":false,"path":"posts/aca7477a.html","link":"","permalink":"http://blog.luodexin.com/posts/aca7477a.html","excerpt":"","text":"1.什么是路由网关网关是系统的唯一的对外入口，介于客户端和服务器端之间的中间层，处理 非业务功能 。 提供路由请求、鉴权、监控、缓存、限流等功能。它将”1对N”问题（一个客户端请求服务器N个服务）转换成了”1对1”问题（一个客户端请求一个网关）。通过服务路由的功能，可以在对外提供服务时，只暴露 网关中配置的调用地址，而调用方就不需要了解后端具体的微服务主机（隐藏后端的实际接口服务，系统更加安全）。 2.为什么需要网关没有网关的架构存在的问题： 客户端会多次请求不同的服务（1对N），增加客户端的复杂性 存在跨域请求，处理相对复杂 客户端访问的很多服务都要做认证，认证系统更加复杂 难以重构，多个服务可能会合并成一个或者拆分成多个 网关的优点：微服务网关介于服务端与客户端的中间层，所有外部服务请求都会先经过微服务网关客户只能跟微服务网关进行交互，无需调用特定微服务接口，简化了开发。服务网关 = 路由转发 + 过滤器（1）路由转发：接收一切外界请求，转发到后端的微服务上去。（2）过滤器：在服务网关中可以完成一系列的横切功能，例如权限校验、限流以及监控等，这些都可以通过过滤器完成（其实路由转发也是通过过滤器实现的) 3.服务网关技术选型引入服务网关后的微服务架构如上，总体包含三部分：服务网关、open-service和service。 （1）总体流程​ 服务网关、open-service和service启动时注册到注册中心上去；​ 用户请求时直接请求网关，网关做智能路由转发（包括服务发现，负载均衡）到open-service，这其中包含权限校验、监控、限流等操作。​ open-service聚合内部service响应，返回给网关，网关再返回给用户 （2）引入网关的注意点 增加了网关，多了一层转发（原本用户请求直接访问open-service即可），性能会下降一些（但是下降不大，通常，网关机器性能会很好，而且网关与open-service的访问通常是内网访问，速度很快） （3）服务网关基本功能 智能路由： 接收外部一切请求，并转发到后端的对外服务open-service上去。 注意：我们只转发外部请求，服务之间的请求不走网关，这就表示全链路追踪、内部服务API监控、内部服务之间调用的容错、智能路由不能在网关完成；当然，也可以将所有的服务调用都走网关，那么几乎所有的功能都可以集成到网关中，但是这样的话，网关的压力会很大，不堪重负。 权限校验 可在微服务网关上进行认证，然后在将请求转发给微服务，无须每个微服务都进行认证，不校验服务内部的请求。 API监控 只监控经过网关的请求，以及网关本身的一些性能指标（例如，gc等） 限流 与监控配合，进行限流操作； API日志统一收集 类似于一个aspect切面，记录接口的进入和出去时的相关日志。 4. Spring-Cloud 搭建 Zuul 网关Zuul 是一个基于JVM的路由器并提供服务器端负载均衡。Spring-Cloud 设计了一个嵌入式的 Zuul 代理用来简化UI应用调用一个或者多个后端服务的应用场景，在用户界面调用后端服务时说非常有用的，它避免了每个服务单独处理跨域请求和认证。 创建Maven工程或者模块 1.Pom配置：12345678910111213141516171819202122232425262728293031&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.6.RELEASE&lt;/version&gt; &lt;relativePath/&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;!-- zuul 依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-zuul&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Eureka 客户端依赖，用于将服务网关注册到Eureka服务器 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;!-- Spring-cloud 依赖配置 --&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 2.应用配置 application.yml12345678910111213141516171819202122# 服务名称spring: application: name: zuul-gatewayserver: port: 5000# 指定注册中心eureka: client: service-url: defaultZone: http://localhost:9999/eureka# 自定义路由映射zuul: routes: consumer: # 请求路径 path: /consumer/** # 转发到的服务 service-id: sc-rest-consumer 其中自定义路由映射的配置方法： 传统路由配置例如所有以myapi开头的url路由至http://127.0.0.1:2000/下 http://127.0.0.1:8888/myapi/hello –&gt; http://127.0.0.1:2000/hello 配置为： zuul.routes.myApi.path=/myapi/**zuul.routes.myApi.url=http://127.0.0.1:2000 forward 模式直接将请求转发至zuul提供的rest服务。Zuul代理所有注册到 EurekaServer 的微服务，路由规则： http://ZUUL_HOST:ZUUL_PORT/微服务实例名(serverId)/** 转发至serviceId对应的微服务。 服务发现模式将路径匹配的请求转发至指定的服务 1234#路由地址zuul.routes.myEureka.path=/eureka/**#为具体服务的名称zuul.routes.myEureka.service-id=eureka-client 3.启动类配置12345678@EnableZuulProxy@SpringBootApplicationpublic class ScZuulApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ScZuulApplication.class, args); &#125;&#125; 先启动 Eureka 服务器应用，然后启动网关应用和服务，之后通过访问网关即可访问内部服务 如通过访问网关 http://localhost:5000/consumer/consumer/api/invitation-feign?userId=1001 即可访问内部发consuer服务，效果和直接访问 http://localhost:8001/consumer/api/invitation-feign?userId=1001相同。","categories":[{"name":"Spring - SpringCloud","slug":"Spring-SpringCloud","permalink":"http://blog.luodexin.com/categories/Spring-SpringCloud/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.luodexin.com/tags/Spring/"},{"name":"Zuul","slug":"Zuul","permalink":"http://blog.luodexin.com/tags/Zuul/"}]},{"title":"Spring AOP 编程","slug":"spring-framework/aop","date":"2019-03-18T06:12:59.000Z","updated":"2020-11-07T11:38:57.243Z","comments":false,"path":"posts/b19a926a.html","link":"","permalink":"http://blog.luodexin.com/posts/b19a926a.html","excerpt":"","text":"1. AOP介绍AOP是能够让我们在不影响原有功能的前提下，为软件横向扩展功能。 那么横向扩展怎么理解呢，我们在WEB项目开发中，通常都遵守三层原则，包括控制层（Controller）-&gt;业务层（Service）-&gt;数据层（dao），那么从这个结构下来的为纵向，它具体的某一层就是我们所说的横向。我们的AOP就是可以作用于这某一个横向模块当中的所有方法。 2. Spring AOP 的常用术语2.1 通知（Advice）通知是定义在切面中的方法，是你想要的具体功能，也就是上面说的 安全、事物、日志等。你给先定义好把，然后在想用的地方用一下。 通知的类型有： 前置通知：@Before 在目标业务方法执行之前执行的方法 后置通知：@After 在目标业务方法执行之后执行的方法 返回通知：@AfterReturning 在目标业务方法返回结果之后执行的方法 异常通知：@AfterThrowing 在目标业务方法抛出异常之后的方法 环绕通知：@Around 功能强大，可代替以上四种通知，还可以控制目标业务方法是否执行以及何时执行 2.2 连接点（JoinPoint）连接点是spring允许你使用通知的地方，那可真就多了，基本每个方法的前、后（两者都有也行）或抛出异常时都可以是连接点，spring只支持方法连接点，其他如aspectJ还可以让你在构造器或属性注入时都行，不过那不是咱关注的，只要记住，和方法有关的前前后后（抛出异常），都是连接点。 2.3 切入点（Pointcut）上面说的连接点的基础上，来定义切入点。在你的一个类里，有15个方法，那就有几十个连接点了（每个方法的之前、之后、异常抛出等），但是你并不想在所有方法附近都使用通知（使用叫织入，后面介绍），你只想在调用其中几个方法之前、之后或者抛出异常时干点什么（具体干点什么就是通知中定义的操作），那么就用切点来定义这几个方法，让切点来筛选连接点，选中那几个你想要的方法。即切点用来选择连接点，一个切点上可能选中多个连接点，每个连接点可以有自己的通知 。 2.4 切面（Aspect）切面是通知和切入点的结合。现在发现了吧，没连接点什么事情，连接点就是为了让你好理解切点，搞出来的，明白这个概念就行了。通知说明了干什么和什么时候干（什么时候是通过方法名中的before、after、around等就能知道），而切入点说明了在哪干（指定到底是哪个方法），这就是一个完整的切面定义。 2.5 引入（introduction）引入允许我们向现有的类添加新方法属性。这不就是把切面（也就是新方法属性：通知定义的）用到目标类中吗 2.6 目标（target）引入中所提到的目标类，也就是要被通知的对象，也就是真正的业务逻辑，他可以在毫不知情的情况下，被咱们织入切面。而自己专注于业务本身的逻辑。 2.7 代理(proxy)怎么实现整套aop机制的，都是通过代理模式（动态代理模式）。 2.8 织入(weaving)把切面应用到目标对象来创建新的代理对象的过程。有3种方式，spring采用的是运行时，为什么是运行时，后面解释。 3. AoP的实现方式AoP 通常使用代理模式来实现，代理模式能够对目标对象进行增强，在调用目标方法之前、之后、返回、异常的时候进行通知。 根据代理对象生成时间的不可，可以分为静态代理和动态代理两类: 静态代理:在源码编时通过手动编码的方式实现代理，当要代理的对象非常多的时候就会有很多重复的工作。 动态代理: 类加载期间动态代理需要特殊的类加载器 运行时动态代理 JDK动态代理 接口代理，需要代理对象实现了接口 CGLib动态代理 子类代理 4. Spring 中使用 AoP在 Spring 使用 AoP 编程，首先需要定义一个切面 1234567891011121314151617181920212223@Component@Aspectpublic class LogAspectJ &#123; // 前置通知 @Before(&quot;execution(* com.test.spring.service..*.*(..))&quot;) public void before(JoinPoint joinPoint) &#123; System.out.println(&quot;开始执行: &quot; + joinPoint.getSignature().getName()); &#125; // 后置通知 @After(&quot;execution(* com.test.spring.service..*.*(..))&quot;) public void after(JoinPoint joinPoint) &#123; System.out.println(&quot;结束执行: &quot; + joinPoint.getSignature().getName()); &#125; // 异常通知 @AfterThrowing(&quot;execution(* com.test.spring.service..*.*(..))&quot;) public void throwing(JoinPoint joinPoint) &#123; System.out.println(&quot;执行异常: &quot; + joinPoint.getSignature().getName()); &#125;&#125; Service层测试业务代码如下: 12345678@Servicepublic class UserServiceImpl implements UserService &#123; @Override public String getUser() &#123; return &quot;User: Admin&quot;; &#125;&#125; 这里实现了 UserService 接口， Spring 默认使用 JDK 动态代理生产代理类. 5. Spring AoP 原理Spring AoP 生产动态代理对象是通过 BeanPostProcessor 来完成的， 具体说是通过AnotationAwareAspectJAutoProxyCreator 来完成的。 通过带源码的调试可知，在Bean 的生命周期中（AbstractAutoWireCapableBeanFactory类中）在实例化 Bean 之后会调用各个BeanPostProcessor的postProcessAfterInitialization 方法。 在调用 AnotationAwareAspectJAutoProxyCreator 的 postProcessAfterInitialization 方法中通过会调用 wrapIfNecessary 方法对需要进行代理的目标对象生成生成代理对象。 12345678910@Overridepublic Object postProcessAfterInitialization(@Nullable Object bean, String beanName) throws BeansException &#123; if (bean != null) &#123; Object cacheKey = getCacheKey(bean.getClass(), beanName); if (this.earlyProxyReferences.remove(cacheKey) != bean) &#123; return wrapIfNecessary(bean, beanName, cacheKey); &#125; &#125; return bean;&#125; 12345678910111213141516171819202122232425protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) &#123; if (StringUtils.hasLength(beanName) &amp;&amp; this.targetSourcedBeans.contains(beanName)) &#123; return bean; &#125; if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) &#123; return bean; &#125; if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) &#123; this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; &#125; // Create proxy if we have advice. Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); if (specificInterceptors != DO_NOT_PROXY) &#123; this.advisedBeans.put(cacheKey, Boolean.TRUE); Object proxy = createProxy( bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; &#125; this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean;&#125;","categories":[{"name":"Spring - SpringFramework","slug":"Spring-SpringFramework","permalink":"http://blog.luodexin.com/categories/Spring-SpringFramework/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.luodexin.com/tags/Spring/"},{"name":"AOP","slug":"AOP","permalink":"http://blog.luodexin.com/tags/AOP/"}]},{"title":"Spring IoC 容器初始化流程","slug":"spring-framework/02.AnnotationConfigApplicationContext","date":"2019-02-08T06:12:59.000Z","updated":"2020-11-07T11:38:57.239Z","comments":false,"path":"posts/73f5d5a4.html","link":"","permalink":"http://blog.luodexin.com/posts/73f5d5a4.html","excerpt":"","text":"使用注解配置的Spring Ioc容器 容器构造方法1234567891011121314151617181920212223242526/** **/public AnnotationConfigApplicationContext() &#123; this.reader = new AnnotatedBeanDefinitionReader(this); this.scanner = new ClassPathBeanDefinitionScanner(this);&#125;public AnnotationConfigApplicationContext(DefaultListableBeanFactory beanFactory) &#123; super(beanFactory); this.reader = new AnnotatedBeanDefinitionReader(this); this.scanner = new ClassPathBeanDefinitionScanner(this);&#125;/** 通过指定的配置类来创建IOC容器，从给定的配置类来创建Bean，并自动刷新上下文 **/public AnnotationConfigApplicationContext(Class&lt;?&gt;... annotatedClasses) &#123; this(); register(annotatedClasses); refresh();&#125;/** 通过指定的包名来创建IOC容器，扫描指定的包来创建Bean，并自动刷新上下文 **/public AnnotationConfigApplicationContext(String... basePackages) &#123; this(); scan(basePackages); refresh();&#125; Spring IoC 容器的组成 AnnotationConfigApplication 中定义了一个 ClassPathBeanDifinitionScanner 和一个 AnnotatedBeanDefinitionReader ，在构造方法中实例化，用了扫描并读取 Bean 定义 父类 GenericApplicationContext 中定义了 DefaultListableBeanFactory， 这是实际上的IOC容器。 DefaultListableBeanFactory 中定义了 beanDefinitionMap， singleObjects, singletonFactories Spring IoC 容器初始化流程 (AnnotationConfigApplicationContext)从AnnotationConfigApplicationContext 的构造函数说起1234567891011public AnnotationConfigApplicationContext(Class&lt;?&gt;... annotatedClasses) &#123; // 实例化 beanFactory, scaner, reader this(); // 注册配置类 register(annotatedClasses); // 完成容器初始化以及 Spring Bean 的构建过程 (核心步骤) // 1. 扫描类(XML配置、 Java配置类、注解类等) (Step 7) // 把类的信息缓存到 bdMap BeanDefinition // 2. 遍历 bdMap ，实例化所有单例类，并放入单例池 (Step 13) refresh();&#125; Spring IoC 容器初始化整体流程 postProcessBeanFactory(beanFactory);这是 AbstractApplicationContext 预留的一个扩展点，默认空实现，此时所有的BeanDefinition已经加载但是尚未实例化。这个扩展点允许 子类实现(自定义的ApplicationContext) 修改应用上下文中的BeanFactory ，例如可以用来实现注册特殊的 BeanPostProcessor 等 invokeBeanFactoryPostProcessors(beanFactory);调用 beanFactory 的后置处理器，完成注册类(用户自定义Bean)的扫描，并封装成 BeanDefinition 放入 BeanDefinitionMap 中。 BeanDefinitionMap 中包含三种 BeanDefinition : RootBeanDifinition: 系统内部Bean AnnotatedGenericBeanDefinition: 用户注解配置的Bean, 例如通过 @Configuration 配置的 ScanedGenericBeanDefinition: 通过包扫描扫描到的 Bean, 例如 扫描到的 @Service 标注类","categories":[{"name":"Spring - SpringFramework","slug":"Spring-SpringFramework","permalink":"http://blog.luodexin.com/categories/Spring-SpringFramework/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.luodexin.com/tags/Spring/"},{"name":"源码","slug":"源码","permalink":"http://blog.luodexin.com/tags/%E6%BA%90%E7%A0%81/"}]},{"title":"Spring Bean的生命周期","slug":"spring-framework/03.bean-lifecycle","date":"2019-02-08T06:12:59.000Z","updated":"2020-11-07T11:38:57.241Z","comments":false,"path":"posts/89080a8e.html","link":"","permalink":"http://blog.luodexin.com/posts/89080a8e.html","excerpt":"","text":"Spring Bean 的生命周期分为四个阶段和多个扩展点。 四个阶段 实例化 Instantiation实例化 Bean 对象 属性赋值 Populate为 Bean 对象的属性赋值 初始化 Initialization完成 Bean 对象的初始化 销毁 Destruction容器关闭时销毁 Bean 多个扩展点 影响多个 Bean BeanPostProcessor InstantiationAwareBeanPostProcessor 影响单个 Bean Aware 类 BeanNameAware BeanClassLoaderAware BeanFactoryAware EnvironmentAware EmbeddedValueResolverAware ApplicationContextAware(ResourceLoaderAware\\ApplicationEventPublisherAware\\MessageSourceAware) 生命周期类 InitializingBean DisposableBean InstantiationAwareBeanPostProcessor 接口继承了 BeanPostProcessor。 其方法 postProcessBeforeInstantiation 在实例化之前调用，如果返回非 null 将跳过实例化过程。其方法 postProcessAfterInstantiation 方法在实例化之后属性赋值之前调用，如果返回 false 将跳过属性赋值过程。 所有的 Aware 扩展都是在初始化阶段之前 1. Spring Bean 生命周期中的四个阶段 实例化 Instantiation 属性赋值 Populate 初始化 Initialization 销毁 Destruction Spring Bean的生命周期可以总结为这四个阶段。把这四个阶段和每个阶段对应的扩展点糅合在一起虽然没有问题，但是这样非常凌乱，难以记忆。要彻底搞清楚Spring的生命周期，首先要把这四个阶段牢牢记住。实例化和属性赋值对应构造方法和setter方法的注入，初始化和销毁是用户能自定义扩展的两个阶段。 前三个阶段主要逻辑都在doCreate()方法中，逻辑很清晰，就是顺序调用以下三个方法，这三个方法与三个生命周期阶段一一对应，非常重要，在后续扩展接口分析中也会涉及。 至于第四阶段销毁，是在容器关闭时调用的，详见ConfigurableApplicationContext#close() 12345678910111213141516171819202122// 忽略了无关代码protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args) throws BeanCreationException &#123; // Instantiate the bean. BeanWrapper instanceWrapper = null; if (instanceWrapper == null) &#123; // 实例化阶段！ instanceWrapper = createBeanInstance(beanName, mbd, args); &#125; // Initialize the bean instance. Object exposedObject = bean; try &#123; // 属性赋值阶段！ populateBean(beanName, mbd, instanceWrapper); // 初始化阶段！ exposedObject = initializeBean(beanName, exposedObject, mbd); &#125; &#125; 2. Spring Bean 生命周期中的扩展点2.1 第一类扩展点(作用于多个Spring bean 的 PostProcessor)第一类扩展点包含两个接口的实现类： InstantiationAwareBeanPostProcessor 和 BeanPostProcessor InstantiationAwareBeanPostProcessor InstantiationAwareBeanPostProcessor 实际上继承了 BeanPostProcessor接口, 所以严格意义上说， InstantiationAwareBeanPostProcessor 的实现类也是一个 BeanPostProcessor 。 InstantiationAwareBeanPostProcessor#postProcessBeforeInstantiation InstantiationAwareBeanPostProcessor 接口中的方法 postProcessBeforeInstantiation 在实例化之前调用。实际上是在 doCreateBean 方法调用之前调用的，也就是在bean实例化之前调用的。 12345678910111213141516171819202122232425@Overrideprotected Object createBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException &#123; try &#123; // Give BeanPostProcessors a chance to return a proxy instead of the target bean instance. // postProcessBeforeInstantiation方法调用点，这里就不跟进了， // for循环调用所有的 InstantiationAwareBeanPostProcessor Object bean = resolveBeforeInstantiation(beanName, mbdToUse); if (bean != null) &#123; return bean; &#125; &#125; try &#123; // 上文提到的doCreateBean方法，可以看到 // postProcessBeforeInstantiation 方法在创建Bean之前调用 Object beanInstance = doCreateBean(beanName, mbdToUse, args); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Finished creating instance of bean &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; return beanInstance; &#125; &#125; InstantiationAwareBeanPostProcessor#postProcessAfterInstantiation 12345678910111213141516171819202122protected void populateBean(String beanName, RootBeanDefinition mbd, @Nullable BeanWrapper bw) &#123; // Give any InstantiationAwareBeanPostProcessors the opportunity to modify the // state of the bean before properties are set. This can be used, for example, // to support styles of field injection. boolean continueWithPropertyPopulation = true; // InstantiationAwareBeanPostProcessor#postProcessAfterInstantiation() // 方法作为属性赋值的前置检查条件，在属性赋值之前执行，能够影响是否进行属性赋值！ if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; if (!ibp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName)) &#123; continueWithPropertyPopulation = false; break; &#125; &#125; &#125; &#125; // 忽略后续的属性赋值操作代码&#125; 可以看到该方法在属性赋值方法内，但是在真正执行赋值操作之前。其返回值为boolean，返回false时可以阻断属性赋值阶段（continueWithPropertyPopulation = false;）。 BeanPostProcessorBeanPostProcessor 中的扩展点 BeanPostProcessor#postProcessBeforeInitialization 和 BeanPostProcessor#postProcessAfterInitialization 穿插在第二类扩展点中进行介绍。 第二类扩展点 (作用于单个Spring bean 的 Aware 接口扩展点)Bean相关 Aware 接口 BeanNameAware BeanClassLoaderAware BeanFactoryAware Bean相关容器接口有三个，分别用来向Bean中注入 Bean的名称、Bean的类加载器、Bean工厂，它们在 Bean 实例化之前调用。 应用上下文相关 Aware 接口 EnvironmentAware EmbeddedValueResolverAware ApplicationContextAware 第二类扩展点调用时机源码分析 123456789101112131415161718192021// 见名知意，初始化阶段调用的方法protected Object initializeBean(final String beanName, final Object bean, @Nullable RootBeanDefinition mbd) &#123; // 这里调用的是三个 Bean相关Aware接口 invokeAwareMethods(beanName, bean); Object wrappedBean = bean; // 这里调用的是应用上下文相关的几个Aware， // 而实质上这里就是前面所说的BeanPostProcessor的调用点！ // 也就是说与Group1中的Aware不同，这里是通过BeanPostProcessor（ApplicationContextAwareProcessor）实现的。 wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); // 调用第三类扩展点: 下文即将介绍的InitializingBean调用点 invokeInitMethods(beanName, wrappedBean, mbd); // 调用第一类扩展点: BeanPostProcessor#postProcessAfterInitialization wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); return wrappedBean;&#125; 第三类扩展点 (生命周期的两个接口: 初始化和销毁) InitializingBean#afterPropertiesSet DisposableBean#destroy InitializingBean 对应生命周期的初始化阶段，在上面源码的invokeInitMethods(beanName, wrappedBean, mbd);方法中调用 有一点需要注意，因为Aware方法都是执行在初始化方法之前，所以可以在初始化方法中放心大胆的使用Aware接口获取的资源，这也是我们自定义扩展Spring的常用方式。除了实现InitializingBean接口之外还能通过注解或者xml配置的方式指定初始化方法，至于这几种定义方式的调用顺序其实没有必要记。因为这几个方法对应的都是同一个生命周期，只是实现方式不同，我们一般只采用其中一种方式。 DisposableBean 类似于InitializingBean，对应生命周期的销毁阶段，以ConfigurableApplicationContext#close()方法作为入口，实现是通过循环取所有实现了DisposableBean接口的Bean然后调用其destroy()方法 。","categories":[{"name":"Spring - SpringFramework","slug":"Spring-SpringFramework","permalink":"http://blog.luodexin.com/categories/Spring-SpringFramework/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.luodexin.com/tags/Spring/"},{"name":"源码","slug":"源码","permalink":"http://blog.luodexin.com/tags/%E6%BA%90%E7%A0%81/"}]},{"title":"Spring源码编译","slug":"spring-framework/01.source-build","date":"2019-02-07T12:17:59.000Z","updated":"2020-11-07T11:38:57.237Z","comments":false,"path":"posts/6b2c22c8.html","link":"","permalink":"http://blog.luodexin.com/posts/6b2c22c8.html","excerpt":"","text":"Spring Framework 源码源码学习的核心、原则、技巧源码学习核心 IOC非常核心但是也同时非常重要，需要关注容器初始化流程、Bean构建流程、Bean循环依赖、后置处理器 AOP 源码学习原则 找到突破口，从一个点开始 先了解整体流程，再抓住主线分析 源码学习技巧 断点调试 常用分析 快速跳转与搜索 快速编译Spring框架源码获取源码 https://github.com/spring-projects/spring-framework 在IDEA中打开源码项目 需要安装gradle插件 直接使用IDEA打开项目 第一次编译很慢，需要下载各种依赖库 加中文注释需要修改编码问题 只需要编译部分模块即可 core context beans aop 向Spring IOC容器注册组件的四种方式方式一: 包扫描+组件标注注解 自动扫描： @ComponentScan(&quot;package-name) 或者 基于XML配置的方法1&lt;context:compoent-scan=&quot;package-name&quot;&gt; 组件注解： @Component @Service @Controller 方式二： 使用 @Bean 注解通常用于在配置类中注册组件 方式三： 使用Spring提供的 FactoryBeanFactoryBean 是工厂类接口(其自身也是IOC的组件，需要加@Component注解)，用户可以通过实现该接口定制实例化 Bean 的逻辑。 用户实现 Spring 提供的 FactoryBean来实现Bean组件的创建逻辑。 方式四： 使用 @Import 注解(在 SpringBoot 中使用较多) @Import(单独类), @Import(类1 ，类2)，批量注册类，手动注册类 批量注册类：实现 ImportSelect 接口 批量注册 手动注册类：实现 ImportBeanDefinitionRegistrar 设计模式 策略模式 Resource 装饰器模式 BeanWrapper 单例模式 代理模式 spring的Proxy模式在aop中有体现，比如JdkDynamicAopProxy和Cglib2AopProxy。 模板方法","categories":[{"name":"Spring - SpringFramework","slug":"Spring-SpringFramework","permalink":"http://blog.luodexin.com/categories/Spring-SpringFramework/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.luodexin.com/tags/Spring/"},{"name":"源码","slug":"源码","permalink":"http://blog.luodexin.com/tags/%E6%BA%90%E7%A0%81/"}]},{"title":"高并发下Redis缓存问题：缓存击穿，缓存穿透，缓存雪崩","slug":"redis/05.高并发下Redis缓存问题","date":"2018-10-11T06:18:53.000Z","updated":"2020-11-07T11:38:57.235Z","comments":false,"path":"posts/454f87ff.html","link":"","permalink":"http://blog.luodexin.com/posts/454f87ff.html","excerpt":"","text":"系统中使用Redis缓存技术能够极大的提升了应用程序的性能和效率，特别是数据查询方面。那么使用Redis做缓存，系统就能保证系统的性能以及高可用吗？答案是否定的，在使用Redis做缓存的系统中仍然存在缓存击穿、缓存穿透、缓存雪崩等经典问题。目前，这几个经典问题在业界也都有比较流行的解决方案。 1. 缓存击穿问题1.1 问题描述缓存击穿，是指一个key非常热点，在不停的扛着大并发，大并发集中对这一个点进行访问，当这个key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就像在一个屏障上凿开了一个洞。 1.2 解决方案一: 使用synchronized+双检查机制此方案适合单机模式的系统，原理类似于单例模式中的懒汉式。代码如下 123456789101112131415161718public String get(String key)&#123; String value = redis.get(key); if(value == null)&#123; //采用synchronized保证一次只有一个请求进入到这个代码块 synchronized (this)&#123; // 再次检查确保一个线程设置了缓存后其他线程不用在查数据库 value = redis.get(key); if(null == value)&#123; value = getValueBySql(key); redis.set(key, value, 1000, TimeUnit.SECONDS); return value; &#125; return value; &#125; &#125;else&#123; return value; &#125;&#125; 1.3 解决方案二: 使用互斥锁(mutex key)业界比较常用的做法就是使用互斥锁。简单地来说，就是在缓存失效的时候（判断拿出来的值为空），不是立即去load db，而是先使用缓存工具的某些带成功操作返回值的操作（比如Redis的SETNX或者Memcache的ADD）去set一个mutex key，当操作返回成功时，再进行load db的操作并回设缓存；否则，就重试整个get缓存的方法。 在使用Redis缓存场景中使用互斥锁: 123456789101112131415161718public String get(key) &#123; String value = redis.get(key); if (value == null) &#123; //查询缓存为空，代表缓存值过期 //设置3min的超时，防止del操作失败的时候，下次缓存过期一直不能load db if (redis.setnx(key_mutex, 1, 3 * 60) == 1) &#123; //代表设置成功 value = db.get(key); redis.set(key, value, expire_secs); redis.del(key_mutex); &#125; else &#123; // 设置失败，代表其他线程获得了锁，其他线程在查询数据库，此时只需要等待一段时间后重试 sleep(50); get(key); // 重试 &#125; &#125; else &#123; return value; &#125; &#125; 2. 缓存穿透问题2.1 问题描述缓存穿透是指查询一个一定不存在的数据（比如查询ID为-1的用户），由于缓存是不命中时被动写的，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大时，可能DB就挂掉了，要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞。 2.2 解决方案一: 缓存层缓存空值将数据库中的空值也缓存到缓存层中，这样查询该空值就不会再访问DB，而是直接在缓存层访问就行。 但是这样有个弊端就是缓存太多空值占用了更多的空间，可以通过给缓存层空值设立一个较短的过期时间来解决，例如60s。 2.3 解决方案二: 布隆过滤器2.3.1 什么是布隆过滤器每个布隆过滤器对应到 Redis 的数据结构里面就是一个大型的位数组和几个不一样的无偏 hash 函数。所谓无偏就是能够把元素的 hash 值算得比较均匀。 向布隆过滤器中添加 key 时，会使用多个 hash 函数对 key 进行 hash 算得一个整数索引值然后对位数组长度进行取模运算得到一个位置，每个 hash 函数都会算得一个不同的位置。再把位数组的这几个位置都置为 1 就完成了 add 操作。 向布隆过滤器询问 key 是否存在时，跟 add 一样，也会把 hash 的几个位置都算出来，看看位数组中这几个位置是否都为 1。只要有一个位为 0，那么说明布隆过滤器中这个 key 不存在。如果都是 1，这并不能说明这个 key 就一定存在，只是极有可能存在，因为这些位被置为 1 可能是因为其它的 key 存在所致。 布隆过滤器可能会误判，如果它说不存在那肯定不存在，如果它说存在，那数据有可能实际不存在；Redis的bitmap只支持2^32大小，对应到内存也就是512MB，误判率万分之一，可以放下2亿左右的数据，性能高，空间占用率及小，省去了大量无效的数据库连接。 如果这个位数组比较稀疏，判断正确的概率就会很大，如果这个位数组比较拥挤，判断正确的概率就会降低。 Redis中布隆过滤器有二个基本指令，bf.add 添加元素，bf.exists 查询元素是否存在，它的用法和 set 集合的 sadd 和 sismember 差不多。注意 bf.add 只能一次添加一个元素，如果想要一次添加多个，就需要用到 bf.madd 指令。同样如果需要一次查询多个元素是否存在，就需要用到 bf.mexists 指令。 2.3.2 使用布隆过滤器处理缓存穿透将数据库中所有的查询条件，放入布隆过滤器中，当一个查询请求过来时，先经过布隆过滤器进行查，如果判断请求查询不存在(一定不存在)，直接丢弃。如果判断请求查询值存在(可能不存在)，则继续查。 2.3.3 布隆过滤器的其它应用 推送系统去重如果历史记录存储在关系数据库里，去重就需要频繁地对数据库进行 exists 查询，当系统并发量很高时，数据库是很难扛住压力的。如果使用缓存把所有历史记录都放入缓存里，占用空间太大明显不现实。使用布隆过滤器能准确过滤掉那些已经看过的内容（将已经看过的记录在布隆过滤器中），那些没有看过的新内容，它也会过滤掉极小一部分 (误判)，但是绝大多数新内容它都能准确识别。这样就可以完全保证推荐给用户的内容都是无重复的。 爬虫去重在爬虫系统中，我们需要对 URL 进行去重，已经爬过的网页就可以不用爬了。但是 URL 太多了，几千万几个亿，如果用一个集合装下这些 URL 地址那是非常浪费空间的。这时候就可以考虑使用布隆过滤器。它可以大幅降低去重存储消耗，只不过也会使得爬虫系统错过少量的页面。 3. 缓存雪崩问题3.1 缓存雪崩问题描述缓存雪崩是由于服务器重启、过期时间设置问题等原因导致缓存大面积失效，进而导致大量的请求直接查询数据库。与缓存击穿的区别在于这里针对很多key缓存，前者则是某一个key。 缓存失效时的雪崩效应对底层系统的冲击非常可怕！大多数系统设计者考虑用加锁或者队列的方式保证来保证不会有大量的线程对数据库一次性进行读写，从而避免失效时大量的并发请求落到底层存储系统上。还有一个简单方案就时讲缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。 3.2 缓存雪崩避免方案一: 请求加锁排队类似于加锁处理缓存穿透，队伪代码如下 12345678910111213141516171819202122public object GetProductListNew() &#123; int cacheTime = 30; String cacheKey = &quot;product_list&quot;; String lockKey = cacheKey; String cacheValue = CacheHelper.get(cacheKey); if (cacheValue != null) &#123; return cacheValue; &#125; else &#123; synchronized(lockKey) &#123; cacheValue = CacheHelper.get(cacheKey); if (cacheValue != null) &#123; return cacheValue; &#125; else &#123; //这里一般是sql查询数据 cacheValue = GetProductListFromDB(); CacheHelper.Add(cacheKey, cacheValue, cacheTime); &#125; &#125; return cacheValue; &#125;&#125; 加锁排队只是为了减轻数据库的压力，并没有提高系统吞吐量。假设在高并发下，缓存重建期间key是锁着的，这是过来1000个请求999个都在阻塞的。同样会导致用户等待超时，这是个治标不治本的方法！ 3.3 缓存雪崩避免方案一: 设置不同的过期时间可以对缓存同时大面积的过期，可以为过期时间做一个均匀分布的处理。比如1-5分钟内，随机分布。 3.3 缓存雪崩避免方案三: 缓存过期标记+异步刷新方案一种使用加锁排队的方法对缓存过期是零容忍的，虽然能够减轻DB压力，防止雪崩。但由于用到了加锁排队，吞吐率是不高的。在高并发场景下可以把条件稍微放宽点：在缓存过期时间后，允许短时间内部分读请求返回旧值，以此兼顾吞吐率。 兼顾吞吐率的方案: 过期时间T到达后，缓存中的key和value不会被清掉，而只是被标记为过期（逻辑上过期，物理上不过期），然后程序异步去刷新缓存。而后续部分读线程在前面的线程刷新cache成功之前，暂时获取缓存中旧的value返回。一旦缓存刷新成功，后续所有线程就能直接获取缓存中新的value。 123456789101112131415161718192021222324252627282930public Object getCacheValue(String key, int expiredTime) &#123; final String signKey = &quot;sign:&quot; + key; // 缓存标记的key Object cacheValue = cache.get(key); if (!isExpired(signKey, false)) &#123; // 缓存标记未过期 return cacheValue; &#125; else &#123; // 缓存标记signKey已过期，异步更新缓存key THREAD_POOL.execute(() -&gt; &#123; try &#123; if (DistributeLock.lock(key)) &#123; if (isExpired(signKey, true)) &#123; // double-check Object cacheValue = GetValueFromDB(); // 读数据库 if (cacheValue != null) &#123; cache.set(key, cacheValue); // 设置缓存 setSign(signKey, expiredTime); // 设置缓存标记 &#125; &#125; &#125; &#125; catch (Exception ex) &#123; logger.error(ex.getMessage(), ex); &#125; finally &#123; DistributeLock.unlock(key); &#125; &#125;); return cacheValue; &#125; &#125;","categories":[{"name":"Redis - 常见问题","slug":"Redis-常见问题","permalink":"http://blog.luodexin.com/categories/Redis-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.luodexin.com/tags/Redis/"}]},{"title":"Java消息队列","slug":"mq/Kafka/01.Java消息队列","date":"2018-10-11T06:18:53.000Z","updated":"2020-11-07T11:38:57.281Z","comments":false,"path":"posts/896736b9.html","link":"","permalink":"http://blog.luodexin.com/posts/896736b9.html","excerpt":"","text":"Java 消息队列1. JMS OverviewJMS (Java Message Service, Java消息服务) 是一个在 Java标准化组织（JCP）内开发的标准（代号JSR 914)。Java消息服务应用程序接口是一个Java平台中面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。 JMS 定义了点对点 和 发布订阅 两种消息服务模式。这两种模式主要区别或解决的问题就是发送到队列的消息能否重复消费(多订阅)。 2. 点对点模型在点对点(point to point， queue)模式中，消息生产者生产消息发送到queue中，然后消息消费者从queue中取出并且消费消息。点对点模式中，消息被消费以后，queue中不再有存储，所以点对点模式中不支持消息的重复消费。 在点对点模式中，Queue本身是支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。 3. 发布订阅模式消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。 4. Pull VS Push4.1 Pull 消费模式生产者将消息放入消息队列后，由消费者主动的去拉取消息进行消费。Pull模式的优点是消费者拉取消息的速率可以由自己控制。但是消息队列是否有消息需要消费，在消费者端无法感知，所以在消费者端需要额外的线程去监控。 Kafka遵循了传统的方式，选择由producer向broker push消息并由consumer从broker pull消息。 4.2 Push 消费模式生产者将消息放入消息队列后，队列会将消息推送给订阅过该类消息的消费者（类似微信公众号）。由于是消费者被动接收推送，所以无需感知消息队列是否有待消费的消息！(问题：消费者的机器性能不同，处理速度不同，消息队列无法感知消费者处理速度的区别，如果推送速度快，处理速度慢的消费者无法接受，造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞，如果推送速度慢，处理速度快的消费者资源浪费)。 Facebook的Scribe和Cloudera的Flume,采用非常不同的push模式。","categories":[{"name":"MQ","slug":"MQ","permalink":"http://blog.luodexin.com/categories/MQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://blog.luodexin.com/tags/MQ/"}]},{"title":"Kafka 介绍","slug":"mq/Kafka/02.Kafka的基本概念","date":"2018-10-11T06:18:53.000Z","updated":"2020-11-07T11:38:57.283Z","comments":false,"path":"posts/924271fe.html","link":"","permalink":"http://blog.luodexin.com/posts/924271fe.html","excerpt":"","text":"Kafka 是由 Linkedin 公司开发的，它是一个分布式的，支持多分区、多副本，基于 Zookeeper 的分布式消息流平台，它同时也是一款开源的基于发布订阅模式的消息引擎系统。 1. Kafka 的基本概念主要术语 消息：Kafka 中的数据单元被称为消息，也被称为记录，可以把它看作数据库表中某一行的记录。 批次：为了提高效率， 消息会分批次写入 Kafka，一个批次就代指的是一组消息。 主题：消息的种类称为 主题（Topic）, 可以说一个主题代表了一类消息。相当于是对消息进行分类。主题就像是数据库中的表。 分区：主题可以被分为若干个分区（partition），同一个主题中的分区可以不在一个机器上，有可能会部署在多个机器上，由此来实现 Kafka 的伸缩性，单一主题中的分区有序，但是无法保证主题中所有的分区有序 生产者：向主题发布消息的应用程序端称为生产者（Producer），生产者用于持续不断的向某个主题发送消息。 消费者：订阅主题消息的客户端程序称为消费者（Consumer），消费者用于处理生产者产生的消息。 消费者群组：生产者与消费者的关系就如同餐厅中的厨师和顾客之间的关系一样，一个厨师对应多个顾客，也就是一个生产者对应多个消费者，消费者群组（Consumer Group）指的就是由一个或多个消费者组成的群体。 偏移量：偏移量（Consumer Offset）是一种元数据，它是一个不断递增的整数值，用来记录消费者发生重平衡时的位置，以便用来恢复数据。 Broker: 一个独立的 Kafka 服务器就被称为 Broker，Broker 接收来自生产者的消息，为消息设置偏移量，并提交消息到磁盘保存。 Broker 集群： Broker是集群的组成部分，Broker 集群由一个或多个 Broker 组成。每个集群都有一个被称为 Leader 的 Broker 充当了集群控制器的角色（自动从集群的活跃成员中选举出来）。 副本：Kafka 中消息的备份又叫做副本（Replica），副本的数量是可以配置的，Kafka 定义了两类副本：领导者副本（Leader Replica） 和 追随者副本（Follower Replica），前者对外提供服务，后者只是被动跟随。 重平衡：Rebalance。消费者组内某个消费者实例挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。Rebalance 是 Kafka 消费者端实现高可用的重要手段。 2. Kafka 的特性（设计原则） 高吞吐、低延迟：Kafka 最大的特点就是收发消息非常快，Kafka 每秒可以处理几十万条消息，它的最低延迟只有几毫秒； 高伸缩性：每个主题(topic) 包含多个分区(partition)，主题中的分区可以分布在不同的主机(Broker)中； 持久性、可靠性：Kafka 能够允许数据的持久化存储，消息被持久化到磁盘，并支持数据备份防止数据丢失，Kafka 底层的数据存储是基于 Zookeeper 存储的，Zookeeper 我们知道它的数据能够持久存储； 容错性：允许集群中的节点失败，某个节点宕机，Kafka 集群能够正常工作； 高并发：支持数千个客户端同时读写 3. Kafka 的使用场景 活动跟踪：Kafka 可以用来跟踪用户行为，比如我们经常回去淘宝购物，你打开淘宝的那一刻，你的登陆信息，登陆次数都会作为消息传输到 Kafka ，当你浏览购物的时候，你的浏览信息，你的搜索指数，你的购物爱好都会作为一个个消息传递给 Kafka ，这样就可以生成报告，可以做智能推荐，购买喜好等； 传递消息：Kafka 另外一个基本用途是传递消息，应用程序向用户发送通知就是通过传递消息来实现的，这些应用组件可以生成消息，而不需要关心消息的格式，也不需要关心消息是如何发送的； 度量指标：Kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告； 日志记录：Kafka 的基本概念来源于提交日志，比如我们可以把数据库的更新发送到 Kafka 上，用来记录数据库的更新时间，通过Kafka以统一接口服务的方式开放给各种consumer，例如hadoop、Hbase、Solr等； 流式处理：流式处理是有一个能够提供多种应用程序的领域； 限流削峰：Kafka 多用于互联网领域某一时刻请求特别多的情况下，可以把请求写入Kafka 中，避免直接请求后端程序导致服务崩溃。 4. Kafka 消息队列消息队列一般分为两种模式：点对点模式和发布订阅模式。Kafka遵循了传统的方式，选择由producer向broker push消息并由consumer从broker pull消息。 4.3 核心API Producer API，它允许应用程序向一个或多个 topics 上发送消息记录； Consumer API，允许应用程序订阅一个或多个 topics 并处理为其生成的记录流； Streams API，它允许应用程序作为流处理器，从一个或多个主题中消费输入流并为其生成输出流，有效的将输入流转换为输出流； Connector API，它允许构建和运行将 Kafka 主题连接到现有应用程序或数据系统的可用生产者和消费者。例如，关系数据库的连接器可能会捕获对表的所有更改。 4.4 Kafka为何如此之快 顺序读写：Kafka 采取顺序写入磁盘的方式，避免了随机磁盘寻址的浪费 零拷贝：Kafka 实现了零拷贝原理来快速移动数据，避免了内核态与用户态的切换。 分批发送：Kafka 可以将数据记录分批发送，从生产者到文件系统（Kafka 主题日志）到消费者，可以端到端的查看这些批次的数据，批处理能够进行更有效的数据压缩并减少 I/O 延迟。 消息压缩： 5. Kafka系统架构系统架构 如上图所示，一个典型的 Kafka 集群中包含若干Producer（可以是web前端产生的Page View，或者是服务器日志，系统CPU、Memory等），若干Broker（Kafka支持水平扩展，一般Broker数量越多，集群吞吐率越高），若干Consumer Group，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行rebalance。Producer使用push模式将消息发布到Broker，Consumer使用pull模式从Broker订阅并消费消息。 6. Kafka消息发送流程 生产者产生消息后将消息Push到Broker的流程如下 Producer在写入数据的时候永远的找leader，不会直接将数据写入follower！消息写入leader后，follower是主动的去leader进行同步的！producer采用push模式将数据发布到Broker，每条消息追加到分区中，顺序写入磁盘，所以保证同一分区内的数据是有序的 Kafka为什么要做分区呢？ 1、 方便扩展。因为一个topic可以有多个partition，所以我们可以通过扩展机器去轻松的应对日益增长的数据量。 2、 提高并发。以partition为读写单位，可以多个消费者同时消费数据，提高了消息的处理效率。 如果某个topic有多个partition，producer又怎么知道该将数据发往哪个partition呢？ 1、partition在写入的时候可以指定需要写入的partition，如果有指定，则写入对应的partition。 2、 如果没有指定partition，但是设置了数据的key，则会根据key的值hash出一个partition。 3、 如果既没指定partition，又没有设置key，则会轮询选出一个partition。 producer在向Kafka写入消息的时候，怎么保证消息不丢失呢？ 通过ACK应答机制！在生产者向队列写入数据的时候可以设置参数来确定是否确认Kafka接收到数据，这个参数可设置的值为0、1、all。 0代表producer往集群发送数据不需要等到集群的返回，不确保消息发送成功。安全性最低但是效率最高。 1代表producer往集群发送数据只要leader应答就可以发送下一条，只确保leader发送成功。 all代表producer往集群发送数据需要所有的follower都完成从leader的同步才会发送下一条，确保leader发送成功和所有的副本都完成备份。安全性最高，但是效率最低。 * 如果往不存在的topic写数据，能不能写入成功呢？ * Kafka会自动创建该不存在的topic，分区和副本的数量根据默认配置都是1。 7. Kafka数据保存Kafka初始会单独开辟一块磁盘空间，顺序写入数据（效率比随机写入高） 7.1 Partition 结构前面说过了每个topic都可以分为一个或多个partition，如果你觉得topic比较抽象，那partition就是比较具体的东西了！Partition在服务器上的表现形式就是一个一个的文件夹，每个partition的文件夹下面会有多组segment文件，每组segment文件又包含.index文件、.log文件、.timeindex文件（早期版本中没有）三个文件， log文件就实际是存储message的地方，而index和timeindex文件为索引文件，用于检索消息。 这个partition有三组segment文件，每个log文件的大小是一样的，但是存储的message数量是不一定相等的（每条的message大小不一致）。文件的命名是以该segment最小offset来命名的，如000.index存储offset为0~368795的消息，Kafka就是利用分段+索引的方式来解决查找效率的问题。 7.2 Message结构 log文件就实际是存储message的地方，我们在producer往Kafka写入的也是一条一条的message 消息主要包含消息体、消息大小、offset、压缩类型……等等 1、 offset：offset是一个占8byte的有序id号，它可以唯一确定每条消息在parition内的位置！ 2、 消息大小：消息大小占用4byte，用于描述消息的大小。 3、 消息体：消息体存放的是实际的消息数据（被压缩过），占用的空间根据具体的消息而不一样。 存储策略 无论消息是否被消费，Kafka都会保存所有的消息。那对于旧数据有什么删除策略呢？ 1、 基于时间，默认配置是168小时（7天）。 2、 基于大小，默认配置是1073741824。 需要注意的是，Kafka读取特定消息的时间复杂度是O(1)，所以这里删除过期的文件并不会提高Kafka的性能！ 8. Kafka数据消费8.1 消息消费消息存储在log文件后，消费者就可以进行消费了。与生产消息相同的是，消费者在拉取消息的时候也是找leader去拉取。 多个消费者可以组成一个消费者组（consumer group），每个消费者组都有一个组id！同一个消费组者的消费者可以消费同一topic下不同分区的数据，并且一个Topic的某一个分区不会被多个消费者消费同时消费！！！ 当消费者组中的消费者数量小于partition的数量：会出现一个消费者消费处理多个partition的情况，如图中的Consumer1，此时Consumer1的处理速度可能不及其他Consumer的处理速度 当消费者组中的消费者数量大于partition的数量：不会出现多个Consumer消费同一个partition的情况，多出来的消费者不消费任何partition的数据 因此： 建议消费者组的consumer的数量与partition的数量一致！ 8.2 消息的查找过程：每个partition划分为多组segment，每个segment又包含.log、.index、.timeindex文件，log文件中存放的每条message包含offset、消息大小、消息体……等数据。 查找消息的时候是怎么利用segment+offset配合查找的呢？假如现在需要查找一个offset为368801的message是什么样的过程呢？ 1、 二分查找确定segment：先找到offset为368801的message所在的segment文件（利用二分法查找），这里找到的就是在第二个segment文件。 2、 根据index索引文件确定消息物理偏移：打开找到的segment中的.index文件（也就是368796.index文件，该文件起始偏移量为368796+1，我们要查找的消息offset为368801，其相对offset为368801-368796=5），由于index索引文件采用的是稀疏索引的方式存储着相对offset及对应message物理偏移量的关系，所以直接找相对offset为5的索引找不到，这里同样利用二分法查找相对offset小于或者等于指定的相对offset的索引条目中最大的那个相对offset，所以找到的是相对offset为4的这个索引。 3、根据物理偏移从log文件中读取消息：根据找到的相对offset为4的索引确定message存储的物理偏移位置为256。打开数据文件，从位置为256的那个地方开始顺序扫描直到找到offset为368801的那条Message 这套消息查找机制是建立在offset为有序的基础上，利用segment+有序offset+稀疏索引+二分查找+顺序查找等多种手段来高效的查找数据！","categories":[{"name":"MQ - Kafka","slug":"MQ-Kafka","permalink":"http://blog.luodexin.com/categories/MQ-Kafka/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://blog.luodexin.com/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"http://blog.luodexin.com/tags/Kafka/"}]},{"title":"Redis 并发写数据一致性","slug":"redis/03.Redis并发写的数据安全问题","date":"2018-10-10T01:36:05.000Z","updated":"2020-11-07T11:38:57.232Z","comments":false,"path":"posts/63919a0b.html","link":"","permalink":"http://blog.luodexin.com/posts/63919a0b.html","excerpt":"","text":"1. Redis 缓存写一致性解决方案1. 问题背景:在某些业务场景下，可能存在多个连接同时对某一个key的缓存进行写操作，这种操作可能是基于key的旧值进行的。例如有两个线程同时对price进行写操作，同时加10，最终结果我们知道，应该为30才是正确，但是实际上可能出现如下的情况。 问题场景:T1时刻，连接1将price读出，目标设置的数据为10+10 = 20。T2时刻，连接2也将数据读出，也是为10，目标设置为20。T3时刻，连接1将price设置为20。T4时刻，连接2也将price设置为20，则最终结果是一个错误值20。 这里的场景中使用了一个price的缓存，在这种简单场景下可以使用 Redis 提供的本地方法INCR, INCRBY 等进行操作来避免数据问题。但是有的场景下可能是一个复杂对象中的一个字段，这时 Redis 本地方法就没没有那么好用了，那么就需要将整个数据读取到内存中进行计算，再将计算结果写入Redis中，这是就可能出现上述的问题了。 2. 方案一: 独占锁 (实现起来较复杂，成本高)3. 方案二: 乐观锁乐观锁本质上是假设不会进行冲突，使用redis的命令 watch 进行构造条件。watch这里表示监控该key值，后面的事务是有条件的执行，如果从watch的exec语句执行时，watch的key对应的value值被修改了，则事务不会执行。 1234567891011watch priceget price $price$price = $price + 10multiset price $priceexec 4. 方式三: 队列在方案2中，如果同时进行有多个请求进行写操作，例如同一时刻有100个请求过来，那么只会有一个最终成功，其余99个全部会失败，效率不高。而且从业务层面，有些是不可接受的场景。例如：大家同时去抢一个红包，如果背后也是用乐观锁的机制去处理，那每个请求后都只有一个人成功打开红包，这对业务是不可忍受的。 在这种情况下，如果想让总体效率最大化，可以采用排队的机制进行。将所有需要对同一个key的请求进行入队操作，然后用一个消费者线程从队头依次读出请求，并对相应的key进行操作。这样对于同一个key的所有请求就都是顺序访问，正常逻辑下则不会有写失败的情况下产生 。从而最大化写逻辑的总体效率。","categories":[{"name":"Redis - 常见问题","slug":"Redis-常见问题","permalink":"http://blog.luodexin.com/categories/Redis-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.luodexin.com/tags/Redis/"}]},{"title":"如何保证 Redis 缓存与数据库双写一致性","slug":"redis/04. Redis数据库双写数据一致性问题","date":"2018-10-10T01:36:05.000Z","updated":"2020-11-07T11:38:57.233Z","comments":false,"path":"posts/1ec0f444.html","link":"","permalink":"http://blog.luodexin.com/posts/1ec0f444.html","excerpt":"","text":"系统中使用Redis缓存技术能够极大的提升了应用程序的性能和效率，特别是数据查询方面。但同时，它也带来了一些问题。其中，最要害的问题，就是数据的一致性问题，从严格意义上讲，这个问题无解。如果对数据的一致性要求很高，那么就不能使用缓存。只要用缓存，就可能会涉及到缓存与数据库双存储双写，只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？ 1. 双写一致性绕不开的问题在不使用读写请求串行化方案时，要保证缓存数据库双写一致性，就绕不开两个问题： 是更新缓存还是删除删除缓存？ 更新数据库后操作缓存(更新或者删除)还是先操作缓存后更新数据库 删除缓存比更新缓存好, 为什么？一般来书，删除缓存比更新缓存更好，删除缓存后下一次读数据的时候才会建立缓存，这是一种懒加载思想。那么更新缓存会有什么问题呢？ 在写多读少的场景下浪费性能有的时候需要频繁修改一个缓存涉及的多个表，缓存也频繁更新，但是缓存不一定频繁被使用到。例如一个缓存涉及的表的字段，在 1 分钟内就修改了 20 次，或者是 100 次，那么缓存更新 20 次、100 次；但是这个缓存在 1 分钟内只被读取了 1 次，有大量的冷数据。实际上，如果你只是删除缓存的话，那么在 1 分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低。用到缓存才去算缓存。 在复杂点的缓存场景，缓存不单单是数据库中一个字段直接取的值，而是要经过一系列的计算。那么，每次写入数据库后，都再次计算写入缓存的值，无疑是浪费性能的。 2. 先更新数据库，再更新缓存 (使用更新缓存，不考虑)2.1 问题一: 并发时会造成双写数据不一致（1）线程A更新了数据库（2）线程B更新了数据库（3）线程B更新了缓存（4）线程A更新了缓存这就出现请求A更新缓存应该比请求B更新缓存早才对，但是因为网络等原因，B却比A更早更新了缓存。这就导致了脏数据，因此不考虑。总得来说就是 线程A比线程B先更新数据库，却比线程B后更新缓存，导致数据不一致 2.2 问题二: 使用更新缓存可能会有性能问题同上述为什么删除缓存比更新缓存好，在写多读少或者缓存依赖其他多变的数据时存在性能问题。 3. 先删缓存，再更新数据库3.1 问题一：并发情况下可能导致数据不一致（1）请求A进行写操作，删除缓存（2）请求B查询发现缓存不存在（3）请求B去数据库查询得到旧值（4）请求B将旧值写入缓存（5）请求A将新值写入数据库 总得来说就是请求A删除了缓存，请求B在请求A更新数据库之前查了数据库写入缓存，导致缓存中有脏数据更严重的是，如果缓存没有过期策略，脏数据将一直存在，直到下一次写操作才有可能消失。 3.1.1 优化方案：双删延时策略（1）先淘汰缓存（2）再写数据库（这两步和原来一样）（3）休眠1秒，再次淘汰缓存 (可能其他请求正读取了脏数据准备写缓存，等待其写入完成。读者应该自行评估自己的项目的读数据业务逻辑的耗时。), 具体休眠时间参考业务读数据的时间，再加个几百毫秒。 次方案可以将1秒内所造成的缓存脏数据，再次删除。 第二次删除可以异步进行，这样请求就不会休眠一段时间，增大吞吐量。 3.2 问题二: 在 MySQL 读写分离架构中，可能存在数据不一致（1）请求A进行写操作，先删除缓存，然后往主库中写入新的数据（2）请求B查询发现缓存不存在（3）请求B去从库查询得到旧值(从库还未来得及同步主库)（4）请求B将旧值写入缓存(脏数据)（5）从库从主库同步了最新的数据 总得来说就是 A请求删缓存，写主库，B请求可能在同步之前从从库查询到旧的数据，并写入缓存。同样如果缓存没有过期策略，脏数据将一直存在，直到下一次写操作才有可能消失。 优化方案还是双删延时策略，休眠时间改为主从同步时延加个几百毫秒。 3.3 问题三：在问题一和问题二中使用双删延时策略，如果第二次删除失败怎么办 提供一个保障的重试机制即可（1）更新数据库数据；（2）缓存因为种种问题删除失败（3）将需要删除的key发送至消息队列（4）自己消费消息，获得需要删除的key（5）继续重试删除操作，直到成功 4. 先更新数据库，再删除缓存4.1 问题一: 缓存刚好失效时进行读写可能出现不一致（1）缓存刚好失效（2）请求A查询数据库，得一个旧值（3）请求B将新值写入数据库（4）请求B删除缓存（5）请求A将查到的旧值写入缓存请求A读数据的时候刚好缓存失效，读到数据库里的数据，请求B更新数据库并删除缓存，请求A就旧的数据写入缓存。 此问题出现概率非常低，必须满足以下条件: 必须满足步骤（3）操作耗时比步骤（2）更短，才能保证步骤（4）先于步骤（5），数据库的读操作的速度远快于写操作的（不然做读写分离干嘛，做读写分离的意义就是因为读操作比较快，耗资源少）。 解决方案 给缓存设置有效时间 采用异步延时删除策略，保证步骤（4）在步骤（5）之后发生 5. 请求串行化如果业务严格要求缓存与数据库必须完全一致，那么可以将读请求和写请求串行化，串到一个内存队列里去。但是这不是一个好的方法。串行化可以保证一定不会出现不一致的情况，但是它也会导致系统的吞吐量大幅度降低，用比正常情况下多几倍的机器去支撑线上的一个请求。所以一般来说，如果允许缓存可以稍微的跟数据库偶尔有不一致的情况，也就是说如果你的系统不是严格要求 “缓存+数据库” 必须保持一致性的话，最好不要做这个方案。","categories":[{"name":"Redis - 常见问题","slug":"Redis-常见问题","permalink":"http://blog.luodexin.com/categories/Redis-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.luodexin.com/tags/Redis/"}]},{"title":"Redis 持久化问题","slug":"redis/02.Redis-dump","date":"2018-10-08T08:12:05.000Z","updated":"2020-11-07T11:38:57.229Z","comments":false,"path":"posts/f514ad5a.html","link":"","permalink":"http://blog.luodexin.com/posts/f514ad5a.html","excerpt":"","text":"1. Redis 持久化方案。 RDB 持久化原理: RDB持久化是指在指定的时间间隔内将内存中的数据集快照写入磁盘，实际操作过程是fork一个子进程，先将数据集写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储。 AOF 持久化原理: AOF持久化以日志的形式记录服务器所处理的每一个写、删除操作，查询操作不会记录，以文本的方式记录，可以打开文件看到详细的操作记录。 2. Redis 两种持久化方案对比2.1 RDB持久化优势 1). 一旦采用该方式，那么你的整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。 2). 对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。 3). 性能最大化。对于Redis的服务进程而言，在开始持久化时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。 4). 相比于AOF机制，如果数据集很大，RDB的启动效率会更高。 2.2 RDB持久化劣势 1). 如果你想保证数据的高可用性，即最大限度的避免数据丢失，那么RDB将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。 2). 由于RDB是通过fork子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是1秒钟。 2.2 AOF持久化优势 1). 该机制可以带来更高的数据安全性，即数据持久性。Redis中提供了3中同步策略，即每秒同步、每修改同步和不同步。事实上，每秒同步也是异步完成的，其效率也是非常高的，所差的是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失。而每修改同步，我们可以将其视为同步持久化，即每次发生的数据变化都会被立即记录到磁盘中。可以预见，这种方式在效率上是最低的。至于无同步，无需多言，我想大家都能正确的理解它。 2). 由于该机制对日志文件的写入操作采用的是append模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容。然而如果我们本次操作只是写入了一半数据就出现了系统崩溃问题，不用担心，在Redis下一次启动之前，我们可以通过redis-check-aof工具来帮助我们解决数据一致性的问题。 3). 如果日志过大，Redis可以自动启用rewrite机制。即Redis以append模式不断的将修改数据写入到老的磁盘文件中，同时Redis还会创建一个新的文件用于记录此期间有哪些修改命令被执行。因此在进行rewrite切换时可以更好的保证数据安全性。 4). AOF包含一个格式清晰、易于理解的日志文件用于记录所有的修改操作。事实上，我们也可以通过该文件完成数据的重建。 2.3 AOF的劣势 1). 对于相同数量的数据集而言，AOF文件通常要大于RDB文件。RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 2). 根据同步策略的不同，AOF在运行效率上往往会慢于RDB。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和RDB一样高效。 二者选择的标准，就是看系统是愿意牺牲一些性能，换取更高的缓存一致性（aof），还是愿意写操作频繁的时候，不启用备份来换取更高的性能，待手动运行save的时候，再做备份（rdb）。rdb这个就更有些 eventually consistent的意思了。 3. 常用配置3.1 RDB持久化配置Redis会将数据集的快照dump到dump.rdb文件中。此外，我们也可以通过配置文件来修改Redis服务器dump快照的频率，在打开6379.conf文件之后，我们搜索save，可以看到下面的配置信息： 123save 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，则dump内存快照。save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，则dump内存快照。save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，则dump内存快照。 3.2 AOF持久化配置在Redis的配置文件中存在三种同步方式，它们分别是： 123appendfsync always # 每次有数据修改发生时都会写入AOF文件。appendfsync everysec # 每秒钟同步一次，该策略为AOF的缺省策略。appendfsync no # 从不同步。高效但是数据不会被持久化。","categories":[{"name":"Redis - 常见问题","slug":"Redis-常见问题","permalink":"http://blog.luodexin.com/categories/Redis-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.luodexin.com/tags/Redis/"}]},{"title":"CentOS 环境下源码安装Redis","slug":"redis/01.Install","date":"2018-10-02T08:12:05.000Z","updated":"2020-11-07T11:54:08.153Z","comments":false,"path":"posts/cfc04602.html","link":"","permalink":"http://blog.luodexin.com/posts/cfc04602.html","excerpt":"","text":"1. 准备Redis源码12345cd /usr/localmkdir software &amp;&amp; cd softwarewget wget http://download.redis.io/releases/redis-4.0.6.tar.gztar -zxvf redis-4.0.6.tar.gzcd redis-4.0.6 2.安装相关的依赖包1yum install -y cpp binutils glibc-kernheaders glibc-common glibc-devel gcc make 3.编译Redis1make 4.安装Reids1234mkdir /usr/local/redismkdir /usr/local/etc/redismake PREFIX=/usr/local/redis installcp redis.conf /usr/local/redis 5.更改Redis配置12345678910111213141516171819vim /usr/local/etc/redis/redis.conf# 修改一下配置# redis以守护进程的方式运行# no表示不以守护进程的方式运行(会占用一个终端) daemonize yes# 客户端闲置多长时间后断开连接，默认为0关闭此功能 timeout 300# 设置redis日志级别，默认级别：notice loglevel verbose# 设置日志文件的输出方式,如果以守护进程的方式运行redis 默认:&quot;&quot; # 并且日志输出设置为stdout,那么日志信息就输出到/dev/null里面去了 logfile stdout# 设置密码授权requirepass &lt;设置密码&gt;# 监听ipbind 127.0.0.1 6.更改环境变量12345vim /etc/profile# 配置文件中追加export PATH=&quot;$&#123;PATH&#125;:/usr/local/redis/bin&quot;# :wq 保存退出source /etc/profile 7.配置启动脚本1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556vim redis# 内容如下#!/bin/bash#chkconfig: 2345 80 90# Simple Redis init.d script conceived to work on Linux systems# as it does use of the /proc filesystem.PATH=/usr/local/bin:/sbin:/usr/bin:/binREDISPORT=6379EXEC=/usr/local/redis/bin/redis-serverREDIS_CLI=/usr/local/redis/bin/redis-cli PIDFILE=/var/run/redis.pidCONF=&quot;/usr/local/etc/redis/redis.conf&quot; case &quot;$1&quot; in start) if [ -f $PIDFILE ] then echo &quot;$PIDFILE exists, process is already running or crashed&quot; else echo &quot;Starting Redis server...&quot; $EXEC $CONF fi if [ &quot;$?&quot;=&quot;0&quot; ] then echo &quot;Redis is running...&quot; fi ;; stop) if [ ! -f $PIDFILE ] then echo &quot;$PIDFILE does not exist, process is not running&quot; else PID=$(cat $PIDFILE) echo &quot;Stopping ...&quot; $REDIS_CLI -p $REDISPORT SHUTDOWN while [ -x $&#123;PIDFILE&#125; ] do echo &quot;Waiting for Redis to shutdown ...&quot; sleep 1 done echo &quot;Redis stopped&quot; fi ;; restart|force-reload) $&#123;0&#125; stop $&#123;0&#125; start ;; *) echo &quot;Usage: /etc/init.d/redis &#123;start|stop|restart|force-reload&#125;&quot; &gt;&amp;2 exit 1esac# 保存退出 8.设置开机启动1234567891011121314# 复制脚本文件到init.d目录下cp redis /etc/init.d/# 给脚本增加运行权限chmod +x /etc/init.d/redis# 查看服务列表chkconfig --list# 添加服务chkconfig --add redis# 配置启动级别chkconfig --level 2345 redis on 9.启动Redis12345678910# 启动systemctl start redis #或者 /etc/init.d/redis start # 停止systemctl stop redis #或者 /etc/init.d/redis stop# 查看redis进程ps -el|grep redis# 端口查看netstat -an|grep 6379","categories":[{"name":"Redis - 常见问题","slug":"Redis-常见问题","permalink":"http://blog.luodexin.com/categories/Redis-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.luodexin.com/tags/Redis/"}]},{"title":"JVM 类加载机制(一)","slug":"java/02.advenced-features/07.JVM类加载机制一","date":"2018-01-26T10:49:12.000Z","updated":"2020-11-09T04:05:16.785Z","comments":false,"path":"posts/3f5dc6f5.html","link":"","permalink":"http://blog.luodexin.com/posts/3f5dc6f5.html","excerpt":"","text":"1. 什么是类加载？在Java开发中，程序员编写的Java源码文件是以.java为后缀的文本文件，源码文件在代码编译后，就会生成JVM（Java虚拟机）能够识别的二进制字节流文件（*.class）。把.class文件中的类描述数据从具体的文件(磁盘文件、网络文件等)加载到内存，并对数据进行校验、转换解析、初始化，使这些数据最终成为可以被JVM直接使用的Java类型，这个过程叫做JVM的类加载。 类加载后类的字节码二进制数据会存储在方法区，并且栈中会创建该类的Class对象，Class对象应用方法去中类的二进制数据。 2. 类的生命周期从类的生命周期而言，一个类包括如下阶段： 2.1 加载类的加载阶段，主要是获取定义此类的二进制字节流，并将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构，最后在Java堆中生成一个代表这个类的java.lang.Class对象作为方法区这些数据的访问入口。相对于类加载过程的其他阶段，加载阶段是开发期可控性最强的阶段。我们可以通过定制不通的类加载器，也就是ClassLoader来控制二进制字节流的获取方式。 2.2 验证验证、准备、解析其实都属于连接阶段，而验证就是连接阶段的第一步。这一阶段主要是为了确保Class文件的字节流中包含的信息复合当前虚拟机的要求，并且不会危害虚拟机自身的安全。主要验证过程包括：文件格式验证，元数据验证，字节码验证以及符号引用验证。 2.3 准备准备阶段正式为类变量（static，静态变量）分配内存并设置初始值。这里的初始值并不是初始化的值，而是数据类型的默认零值。这里提到的类变量是被static修饰的变量，而不是实例变量。关于准备阶段为类变量设置零值的唯一例外就是当这个类变量同时也被final修饰，那么在编译时，就会直接为这个常量赋上目标值。 2.4 解析解析时虚拟机将常量池中的符号引用替换为直接引用的过程。 2.5 初始化在类加载机制过程中，类初始化是最后一步。在类加载机制过程中，除了加载阶段用户可以通过自定义的类加载器参与，其他阶段都完全由虚拟机主导和控制，直到到了初始化阶段才真正执行Java代码。 在连接中的准备阶段，静态变量已经赋过一次系统要求的初始值，在初始化阶段，则是根据程序员通过程序的主观计划初始化类变量和其他资源，类的初始化的主要工作是为静态变量赋程序设定的初始值。 Java虚拟机规范中严格规定了有且只有五种情况必须对类进行初始化： 使用new字节码指令创建类的实例，或者使用getstatic、putstatic读取或设置一个静态字段的值（放入常量池中的常量除外），或者调用一个静态方法的时候，对应类必须进行过初始化。 通过java.lang.reflect包的方法对类进行反射调用的时候，如果类没有进行过初始化，则要首先进行初始化。 当初始化一个类的时候，如果发现其父类没有进行过初始化，则首先触发父类初始化。 当虚拟机启动时，用户需要指定一个主类（包含main()方法的类），虚拟机会首先初始化这个类。 使用jdk1.7的动态语言支持时，如果一个java.lang.invoke.MethodHandle实例最后的解析结果REF_getStatic、REF_putStatic、RE_invokeStatic的方法句柄，并且这个方法句柄对应的类没有进行初始化，则需要先触发其初始化。 虚拟机规范使用了“有且只有”这个词描述，这五种情况被称为“主动引用”, 除了上面这5种方式，所有引用类的方式都不会触发初始化，称为”被动引用“。 被动引用例子： 例1. 通过子类引用父类的静态字段，不会导致子类初始化； 1234567891011121314151617181920212223public class SuperClass &#123; //静态变量value public static int value = 666; //静态块，父类初始化时会调用 static&#123; System.out.println(&quot;父类初始化！&quot;); &#125; &#125; //子类 public class SubClass extends SuperClass&#123; //静态块，子类初始化时会调用 static&#123; System.out.println(&quot;子类初始化！&quot;); &#125; &#125; //主类、测试类 public class NotInit &#123; public static void main(String[] args)&#123; System.out.println(SubClass.value); &#125; &#125; 例2. 通过数组定义来引用类，不会触发此类的初始化； 通过数组来引用类，不会触发类的初始化，因为是数组new，而类没有被new，所以没有触发任何“主动引用”条款，属于“被动引用” 1234567891011121314151617//父类 public class SuperClass &#123; //静态变量value public static int value = 666; //静态块，父类初始化时会调用 static&#123; System.out.println(&quot;父类初始化！&quot;); &#125; &#125; //主类、测试类 public class NotInit &#123; public static void main(String[] args)&#123; // 不会导致SuperClass类初始化 SuperClass[] test = new SuperClass[10]; &#125; &#125; 例3. 引用类的静态常量不会触发定义常量的类的初始化，因为常量在编译阶段已经被放到常量池中了。 在NotInit类中引用ConstClass的常量，并不会导致ConstClass的初始化 123456789101112131415//常量类 public class ConstClass &#123; static&#123; System.out.println(&quot;常量类初始化！&quot;); &#125; public static final String HELLOWORLD = &quot;hello world!&quot;; &#125; //主类、测试类 public class NotInit &#123; public static void main(String[] args)&#123; System.out.println(ConstClass.HELLOWORLD); &#125; &#125; 3. 常用类加载器Java中类加载器包含两大类，一类是JVM内置的类加载器，一类是用户自定义的类加载器。JVM内置类加载器包含三个: 根类加载器、扩展类加载器、系统类加载器； 用户自定义类加载器由用户自定义，定义用户类加载器是需要扩展 ClassLoader 类。 3.1 根类加载器根类加载器又称为引导类加载器(bootstrap class loader)、启动类加载器，是用C++语言编写。根类加载器主要负载加载classpath指定的核心类库(JAVA_HOME\\jre\\lib)或者sun.boot.class.path指定路径下的核心类库，出于安全考虑，根类加载器只加载java, javax, sun 开头的类。根类加载器无法被应用程序直接使用，在程序中尝试获取根类加载器会返回null。 3.2 扩展类加载器扩展类加载器ExtensionClassLoader是用JAVA编写，且它的父类加载器是Bootstrap。扩展类加载器的全类名是sun.misc.Launcher$ExtClassLoader（在JDK9中为PlatformClassLoader）。 扩展类加载器负责加载JAVA_HOME/jre/lib/ext目录中的类库或java.ext.dirs指定目录的类库 3.3 系统类加载器系统类加载器也称为应用类加载器(AppClassLoader)，是由Java实现，全类名是sun.misc.Launcher$AppClassLoader, 它的父类是扩展类加载器。系统类加载器负责从classpath环境变量或者java.class.path指定的目录加载类，是用户自定义类加载器的默认父类加载器。 3.4 用户自定义类加载器用户自定义类加载器需要继承java.lang.ClassLoader类，在该类中有四个核心方法： loadClass：双亲委派实现代码，用户自定义类加载器一般不覆盖本方法，方法返回Class findClass：本类加载器加载，用户自定义类加载器应该重写的方法，本方法配合 defineClass 方法使用，得到 Class 对象 defineClass：将字节码解析为JVM能识别的Class对象 resolveClass：用来连接类","categories":[{"name":"Java - JVM","slug":"Java-JVM","permalink":"http://blog.luodexin.com/categories/Java-JVM/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.luodexin.com/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"http://blog.luodexin.com/tags/JVM/"}]},{"title":"JVM垃圾收集器","slug":"java/05.GC","date":"2018-01-15T01:27:36.000Z","updated":"2020-11-07T11:55:36.182Z","comments":false,"path":"posts/b38f68fb.html","link":"","permalink":"http://blog.luodexin.com/posts/b38f68fb.html","excerpt":"","text":"1. 引用类型 强引用 软引用 弱引用 虚引用 1.1 软引用类型 特性一: 在栈空间够时，GC不会去回收软引用对象 特性二: 空间不够时，GC会尝试回收软引用对象 应用场景: 用做缓存 1.2 弱引用 特性一: 只能生存到下一次GC，下一次GC是一定会被回收 应用场景: ThreadLocal @Transaction 事务中, m 方法调用 m1, m2, 同一个线程拿到同一个connection 访问多层调用的时候，多个方法需要访问一个对象，可以在方法中通过参数一层层传递，但是比较复杂，可以考虑 ThreadLocal， 方法调用栈在同一个线程，只能在ThreadLocal中拿到同一个对象。 ThreadLocal每一个Thread类中有一个ThreadLocalMap的对象, 使用ThreadLocal的set的时候，先获取当前线程对象，然后获取当前线程对象里面的ThreadLocalMap对象，然后以ThreadLocal作为key, set() 对象作为value, 放入到当前线程的 ThreadLocalMap。 12345678public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125; 避免内存泄漏的措施: key 使用弱引用，在 threadLocal = null 时, key 能够被回收 在不使用了的时候，使用 remove 来删除 1.3 虚引用 特性一: 无法通过引用获取到对象 特性二: 唯一的作用是在GC回收时得到一个通知 1PhantomRefernce&lt;M&gt; = new PhantomReference&lt;&gt;(new M(), QUEUE) 应用场景: 管理直接内存","categories":[{"name":"Java - JVM","slug":"Java-JVM","permalink":"http://blog.luodexin.com/categories/Java-JVM/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.luodexin.com/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"http://blog.luodexin.com/tags/JVM/"}]},{"title":"linux awk 命令","slug":"linux/cmd/awk","date":"2017-12-28T12:23:21.000Z","updated":"2020-11-07T11:53:16.811Z","comments":false,"path":"posts/d79e147e.html","link":"","permalink":"http://blog.luodexin.com/posts/d79e147e.html","excerpt":"","text":"1.Introductionawk是行处理器 , 相比较屏幕处理的优点，在处理庞大文件时不会出现内存溢出或是处理缓慢的问题，通常用来格式化文本信息, 处理过程是对每一行进行处理, 然后执行符合条件的命令 2. Usage1awk [-F|-f|-v] ‘BEGIN&#123;&#125; condition1 &#123;command1;&#125; condition2 &#123;command2&#125; END&#123;&#125;’ file 可选参数:[-F|-f|-v] -F指定分隔符, 如 -F “:” , 可以指定多个分割符, 如 -F’[:#/]’ 定义三个分隔符-f调用脚本-v定义变量, 如 -v var=value‘ ‘ 引用代码块BEGIN 初始化代码块，在对每一行进行处理之前，初始化代码，主要是引用全局变量，设置FS分隔符 BEGIN {FS=”:”}condition1: 条件1{command1} 命令代码块，包含一条或多条命令, 多条每条以”;”或者换行结束； 多条命令使用分号分隔END 结尾代码块，在对每一行进行处理之后再执行的代码块，主要是进行最终计算或输出结尾摘要信息$0 表示整个当前行$1 每行第一个字段NF 字段数量变量NR 每行的记录号，多文件记录递增FNR 与NR类似，不过多文件记录不递增，每个文件都从1开始FS BEGIN时定义分隔符RS 输入的记录分隔符， 默认为换行符(即文本是按一行一行输入)OFS 输出字段分隔符， 默认也是空格，可以改为制表符等ORS 输出的记录分隔符，默认为换行符,即处理结果也是一行一行输出到屏幕 3. Examples1234#将每一行的前二个字段，分行输出awk -F: &#x27;&#123;print $1; print $2&#125;&#x27; /etc/passwd #输出字段1,3,6，以制表符作为分隔符awk -F: &#x27;&#123;print $1,$3,$6&#125;&#x27; OFS=&quot;\\t&quot; /etc/passwd 123456#统计行数awk &#x27;BEGIN&#123;X=0&#125; /^$/&#123; X+=1 &#125; END&#123;print &quot;I find&quot;,X,&quot;blank lines.&quot;&#125;&#x27; test I find 4 blank lines.#计算文件夹大小ls -l|awk &#x27;BEGIN&#123;sum=0&#125; !/^d/&#123;sum+=$5&#125; END&#123;print &quot;total size is&quot;,sum&#125;&#x27; 1ps -ef | grep wechat | grep -v grep | awk &#x27;&#123;print $2&#125;&#x27;| xargs kill -9 匹配//纯字符匹配 !//纯字符不匹配 //字段值匹配 !//字段值不匹配 ~/a1|a2/字段值匹配a1或a2 123456789101112awk &#x27;/mysql/&#x27; /etc/passwd awk &#x27;/mysql/&#123;print &#125;&#x27; /etc/passwd awk &#x27;/mysql/&#123;print $0&#125;&#x27; /etc/passwd #三条指令结果一样 awk &#x27;!/mysql/&#123;print $0&#125;&#x27; /etc/passwd #输出不匹配mysql的行 awk &#x27;/mysql|mail/&#123;print&#125;&#x27; /etc/passwd awk &#x27;!/mysql|mail/&#123;print&#125;&#x27; /etc/passwd awk -F: &#x27;/mail/,/mysql/&#123;print&#125;&#x27; /etc/passwd //区间匹配 awk &#x27;/\\[2]\\[7][7]*/&#123;print $0&#125;&#x27; /etc/passwd //匹配包含27为数字开头的行，如27，277，2777... awk -F: &#x27;$1~/mail/&#123;print $1&#125;&#x27; /etc/passwd //$1匹配指定内容才显示 awk -F: &#x27;&#123;if($1~/mail/) print $1&#125;&#x27; /etc/passwd //与上面相同 awk -F: &#x27;$1!~/mail/&#123;print $1&#125;&#x27; /etc/passwd //不匹配 awk -F: &#x27;$1!~/mail|mysql/&#123;print $1&#125;&#x27; /etc/passwd more Linux awk 命令详解","categories":[{"name":"Linux - 常用命令","slug":"Linux-常用命令","permalink":"http://blog.luodexin.com/categories/Linux-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.luodexin.com/tags/Linux/"}]},{"title":"Linux下权限管理总结","slug":"linux/cmd/permission","date":"2017-12-28T11:23:21.000Z","updated":"2020-11-07T11:53:23.232Z","comments":false,"path":"posts/e945cd7.html","link":"","permalink":"http://blog.luodexin.com/posts/e945cd7.html","excerpt":"","text":"UNIX下关于文件权限的表示方法和解析 SUID 是 Set User ID, SGID 是 Set Group ID的意思。 9 8 7 6 5 4 3 2 1 0 - r w x r - x r - x 第9位表示文件类型,可以为p、d、l、s、c、b和-： p表示命名管道文件 d表示目录文件 l表示符号连接文件 -表示普通文件 s表示socket文件 c表示字符设备文件 b表示块设备文件 第8-6位、5-3位、2-0位分别表示文件所有者的权限，同组用户的权限，其他用户的权限，其形式为rwx： 常用权限linux系统内有档案有三种身份 u:所用者 g:所在组 o:其他人这些身份对于文档常用的有下面权限：r：读权限，对于文件而言，用户可以读取文档的内容，如用cat，more查看；对于目录来说，用户具有浏览目录的权限w：写权限，对于文件而言，用户可以编辑文件内容；对于目录，用户具有删除、移动目录的权限x：对于文件而言，用户具有执行文件的权限；对于目录而言，用户具有进入目录的权限 其他权限除了读写执行权限外系统还支持强制位（s权限）和粘滞位（t权限) s权限s权限： 设置使文件在 执行阶段 具有文件所有者的权限，相当于临时拥有文件所有者的身份. 典型的文件是 /usr/bin/passwd . 如果一般用户执行该文件, 则在执行过程中, 该文件可以获得root权限, 从而可以更改用户的密码. 12ll -al /usr/bin/passwd -rwsr-xr-x 1 root root 54256 5月 17 2017 /usr/bin/passwd* 可以通过字符模式设置s权限：chmod a+s filename也可以使用绝对模式进行设置：设置suid：将相应的权限位之前的那一位设置为4；设置guid：将相应的权限位之前的那一位设置为2；两者都置位：将相应的权限位之前的那一位设置为4+2=6。注意：在设置s权限时 文件属主、属组必须先设置相应的x权限 ，否则s权限并不能正真生效（chmod命令不进行必要的完整性检查，即使不设置x权限就设置s权限，chmod也不会报错，当我们 ls -l时看到rwS ，大写S说明s权限未生效） t权限t权限：要删除一个文档，您不一定要有这个文档的写权限，但您一定要有这个文档的上级目录的写权限。也就是说，您即使没有一个文档的写权限，但您有这个文档的上级目录的写权限，您也能够把这个文档给删除，而假如没有一个目录的写权限，也就不能在这个目录下创建文档。怎样才能 使一个目录既能够让任何用户写入文档，又不让用户删除这个目录下他人的文档 ，t权限就是能起到这个作用。t权限一般只用在目录上，用在文档上起不到什么作用。在一个目录上设了t权限位后，（如/home，权限为1777)任何的用户都能够在这个目录下创建文档，但只能删除自己创建的文档(root除外)，这就对任何用户能写的目录下的用户文档启到了保护的作用。可以通过chmod +t filename 来设置t权限常用的带粘滞位的文件夹 /tmp , /var/tmp","categories":[{"name":"Linux - 常用命令","slug":"Linux-常用命令","permalink":"http://blog.luodexin.com/categories/Linux-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.luodexin.com/tags/Linux/"}]},{"title":"Java NIO","slug":"java/02.advenced-features/04.NIO","date":"2017-12-20T12:17:59.000Z","updated":"2020-11-07T11:52:46.349Z","comments":false,"path":"posts/94795185.html","link":"","permalink":"http://blog.luodexin.com/posts/94795185.html","excerpt":"","text":"1. NIO 总览NIO 即 New IO（non-blocking IO），这个库是在JDK1.4中才引入的。NIO和IO有相同的作用和目的，但实现方式不同。NIO主要用到的是块（面向缓冲区，或者面向块编程的），所以NIO的效率要比IO高很多。在Java API中提供了两套NIO，一套是针对标准输入输出NIO，另一套就是网络编程NIO。 1.1 NIO三大核心原理 1.1.1 缓冲区(Buffer)缓冲区本质上是一个可以读写数据的内存块，可以理解成是一个容器对象(含数组)，该对象提供了一组方法，可以更轻松地使用内存块，缓冲区对象内置了一些机制，能够跟踪和记录缓冲区的状态变化情况。Channel 提供从文件、网络读取数据的渠道，但是读取或写入的数据都必须经由 Buffer。在NIO厍中，所有数据都是用缓冲区处理的。在读取数据时，它是直接读到缓冲区中的; 在写入数据时，写入到缓冲区中。任何时候访问NIO中的数据，都是通过缓冲区进行操作。 Buffer 是一个顶层父类，它是一个抽象类, 每一种Java基本类型（除了Boolean类型）都对应有一种缓冲区。常用的子类有 ByteBuffer, ShortBuffer, CharBuffer, IntBuffer, LongBuffer, FloatBuffer, DoubleBuffer(抽象类), 对应有具体实现类HeapByteBuffer，HeapByteBufferR, HeapShortBuffer, HeapShortBufferR, HeapCharBuffer, HeapCharBufferR, HeapIntBuffer, HeapIntBufferR…… ByteBuffer: 抽象类， 其 allocate 方法返回 HeapByteBuffer, allocateDirect 方法返回 DirectByteBuffer HeapByteBuffer: ByteBuffer的子类，是在jvm虚拟机的堆上申请内存空间 HeapByteBufferR: HeapByteBuffer的子类，表示只读的 HeapByteBuffer , 当调用写方法会抛出 ReadOnlyBufferException DirectByteBuffer: ByteBuffer的子类，是直接在物理内存中申请内存空间 MappedByteBuffer: NIO 还提供了 MappedByteBuffer， 可以让文件直接在内存(堆外的内存)中进行修改(具体实现类DirectByteBuffer和DirectByteBufferR) Buffer类定义了所有的缓冲区都具有的四个属性来提供关于其所包含的数据元素的信息 mark(标记)， position(位置，下一个要被读写的元素的索引，每次读写缓冲区都会改变该值，为下次读写准备)，limit(表示缓冲区当前的终点，不能对缓冲区超过极限位置进行读写操作)， capacity(容量，即能够容纳的最大数据量，在缓冲区被创建时初始化且不能被修改) Buffer 常用方法 reset() : 将position设置到当前的mark的位置，即从当前mark开始处理 clear(): position设置为0，limit设置为capacity, mark设置为-1 ，即清除4个位置属性 flip(): limit设置为position, position设置为0， mark设置为-1 ，即调用后指针在缓存头不，并设置最多读取已写入的数据 rewind(): position设置为0，mark设置为-1(被忽略) 获取缓冲区 12345// 使用静态方法获取ByteBuffer buf = ButeBuffer.allocate(1024);// 通过channel获取直接缓冲区MappedByteBuffer mappedBuffer = fileChannel.map(MapMode.READ_WRITE, 0, 1024); 1.1.2 通道(Channel)NIO 通过Channel（通道） 进行IO读写, 通道表示IO源与目标之间的连接。通道是双向的，可读也可写，而流的读写是单向的。无论读写，通道不能直接操作数据，通道只能和Buffer交互。因为 Buffer，通道可以异步地读写。 常用的通道： FileChannel SocketChannel ServerSocketChannel DatagramChannel 获取通道的方式 Java中支持通道的类提供了 getChannel() 方法 本地IO：FileInputStream#getChannel() / FileOutputStreamRandomAccessFile 网络IO：Socket, ServerSocket, DatagramSocket JDK 1.7 的NIO2 针对各个通道提供 open 静态方法 JDK 1.7 的NIO2 中 Files 工具类提供 newByteChannel() 方法获取通道 1.1.3 选择器(Selector)Java NIO的选择器允许一个单独的线程来监视多个输入通道，你可以将多个通道注册到同一个选择器上(注册的时候可以选择需要监听的事件，即选择器键, 包括 OP_CONNECT, OP_ACCEPT, OP_READ, OP_WRITE )，然后使用一个单独的线程来“选择”通道：这些通道里已经有可以处理的输入，或者选择已准备写入的通道。这种选择机制，使得一个单独的线程很容易来管理多个通道。 2. NIO与IO的对比 面向流与面向缓冲Java IO 和 NIO之间第一个最大的区别是，IO是面向流的，NIO是面向缓冲区的。 Java IO面向流意味着每次从流中读一个或多个字节，直至读取所有字节，它们没有被缓存在任何地方。此外，它不能前后移动流中的数据。如果需要前后移动从流中读取的数据，需要先将它缓存到一个缓冲区。 *Java NIO的缓冲导向方法略有不同，数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动**。这就增加了处理过程中的灵活性（需要检查是否该缓冲区中包含所有您需要处理的数据）。而且，需确保当更多的数据读入缓冲区时，不要覆盖缓冲区里尚未处理的数据。 阻塞与非阻塞IOJava IO的各种流是阻塞的。这意味着，当一个线程调用 read() 或 write() 时，该线程被阻塞，直到有一些数据被读取或数据完全写入。该线程在此期间不能再干任何事情了。Java NIO的非阻塞模式，使一个线程向某通道请求读取数据时，它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取，而不是保持线程阻塞，所以直至数据变为可以读取之前，该线程可以继续做其他的事情，非阻塞写操作也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。线程通常将非阻塞IO的空闲时间用于在其它通道上执行IO操作，所以一个单独的线程现在可以管理多个输入和输出通道（channel）。 选择器（Selectors）Java NIO 的选择器允许一个单独的线程来监视多个输入通道，你可以注册多个通道使用一个选择器，然后使用一个单独的线程来“选择切换”通道：这些通道里已经有可以处理的输入，或者选择已准备写入的通道。这种选择机制，使得一个单独的线程很容易来管理多个通道。 使用场景对比IO： 如果只有少量的连接，而这些连接每次要发送大量的数据，这时候传统的IO更合适。NIO：如果需要管理同时打开的成千上万个连接，这些连接每次只是发送少量的数据，例如聊天服务器，这时候用NIO处理数据可能是个很好的选择。 3. 服务器通信模型3.1 传统 BIO 通信模型(一请求一应答通信模型)传统 BIO 通信模型是同步阻塞I/O模式，数据的读取写入必须阻塞在一个线程内等待其完成。 采用 BIO 通信模型的服务端，通常由一个独立的 Acceptor 线程负责监听客户端的连接。我们一般通过在 while(true)循环中服务端会调用 accept() 方法等待接收客户端的连接的方式监听请求，请求一旦接收到一个连接请求，就可以建立通信套接字在这个通信套接字上进行读写操作，此时不能再接收其他客户端连接请求，只能等待同当前连接的客户端的操作执行完成， 不过可以通过多线程来支持多个客户端的连接，如上图所示。 如果要让 BIO 通信模型 能够同时处理多个客户端请求，就必须使用多线程（主要原因是 socket.accept() 、 socket.read() 、 socket.write() 涉及的三个主要函数都是同步阻塞的, 主要是执行系统调用的时候是阻塞的，accept(3) 阻塞， write 系统调用阻塞），也就是说它在接收到客户端连接请求之后为每个客户端创建一个新的线程进行链路处理，处理完成之后，通过输出流返回应答给客户端，线程销毁。这就是典型的 一请求一应答通信模型 。我们可以设想一下如果这个连接不做任何事情的话就会造成不必要的线程开销，不过可以通过 线程池机制改善，线程池还可以让线程的创建和回收成本相对较低。使用 FixedThreadPool 可以有效的控制了线程的最大数量，保证了系统有限的资源的控制，实现了N(客户端请求数量):M(处理客户端请求的线程数量)的伪异步I/O模型（N 可以远远大于 M），下面一节”伪异步 BIO”中会详细介绍到。 服务器端实现代码: 1234567891011121314151617181920212223242526272829public class IOServer &#123; public static void main(String[] args) throws IOException &#123; // 服务端处理客户端连接请求 ServerSocket serverSocket = new ServerSocket(3333); // 创建一个Acceptor线程, 接收到客户端连接请求之后为每个客户端创建一个新的线程进行链路处理 new Thread(() -&gt; &#123; while (true) &#123; try &#123; // 阻塞方法获取新的连接 Socket socket = serverSocket.accept(); // 每一个新的连接都创建一个线程，负责读取数据 new Thread(() -&gt; &#123; try &#123; int len; byte[] data = new byte[1024]; InputStream inputStream = socket.getInputStream(); // 按字节流方式读取数据 while ((len = inputStream.read(data)) != -1) &#123; System.out.println(new String(data, 0, len)); &#125; &#125; catch (IOException e) &#123; &#125; &#125;).start(); &#125; catch (IOException e) &#123; &#125; &#125; &#125;).start(); &#125;&#125; 问题 :在 Java 虚拟机中，线程是宝贵的资源，线程的创建和销毁成本很高，除此之外，线程的切换成本也是很高的。尤其在 Linux 这样的操作系统中，线程本质上就是一个进程，创建和销毁线程都是重量级的系统函数。如果并发访问量增加会导致线程数急剧膨胀可能会导致线程堆栈溢出、创建新线程失败等问题，最终导致进程宕机或者僵死，不能对外提供服务。 3.2 伪异步 IO 通信模型为了解决同步阻塞I/O面临的一个链路需要一个线程处理的问题，后来有人对它的线程模型进行了优化一一一后端通过一个线程池来处理多个客户端的请求接入，形成客户端个数M：线程池最大线程数N的比例关系，其中M可以远远大于N.通过线程池可以灵活地调配线程资源，设置线程的最大值，防止由于海量并发接入导致线程耗尽。 采用线程池和任务队列可以实现一种叫做伪异步的 I/O 通信框架，它的模型图如上图所示。当有新的客户端接入时，将客户端的 Socket 封装成一个 Task（该任务实现java.lang.Runnable接口）投递到后端的线程池中进行处理，JDK 的线程池维护一个消息队列和 N 个活跃线程，对消息队列中的任务进行处理。由于线程池可以设置消息队列的大小和最大线程数，因此，它的资源占用是可控的，无论多少个客户端并发访问，都不会导致资源的耗尽和宕机。 伪异步I/O通信框架采用了线程池实现，因此避免了为每个请求都创建一个独立线程造成的线程资源耗尽问题。不过因为它的底层任然是同步阻塞的BIO模型，因此无法从根本上解决问题。 1234567891011121314151617181920212223242526272829public class IOServer &#123; public static void main(String[] args) throws IOException &#123; // 服务端处理客户端连接请求 ServerSocket serverSocket = new ServerSocket(3333); // 创建一个Acceptor线程, 接收到客户端连接请求之后为每个客户端创建一个新的线程进行链路处理 new Thread(() -&gt; &#123; while (true) &#123; try &#123; // 阻塞方法获取新的连接 Socket socket = serverSocket.accept(); // 每一个新的连接都创建一个线程，负责读取数据 new Thread(() -&gt; &#123; try &#123; int len; byte[] data = new byte[1024]; InputStream inputStream = socket.getInputStream(); // 按字节流方式读取数据 while ((len = inputStream.read(data)) != -1) &#123; System.out.println(new String(data, 0, len)); &#125; &#125; catch (IOException e) &#123; &#125; &#125;).start(); &#125; catch (IOException e) &#123; &#125; &#125; &#125;).start(); &#125;&#125; 问题:在活动连接数不是特别高（小于单机1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的 I/O 并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量。 3.3 NIO 非阻塞通信模型NIO 通信模型是一种同步非阻塞的模型，NIO提供了与传统BIO模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发。在NIO非阻塞模型中，将 ServerSocketChannel 注册到一个选择器上，通过轮询可以取到处理客户端连接是 SocketChannel。 将 SocketChannel 注册到客户端处理选择器上。在新的线程中可以通过轮询客户端处理选择器，选中已经就绪的通道进行与客户端通信。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class NIOServer &#123; public static void main(String[] args) throws IOException &#123; // 1. serverSelector 负责轮询是否有新的连接，服务端监测到新的连接之后，不再创建一个新的线程， // 而是直接将新连接绑定到 clientSelector上，这样就不用 IO 模型中 1w 个 while 循环在死等 Selector serverSelector = Selector.open(); // 2. clientSelector负责轮询连接是否有数据可读 Selector clientSelector = Selector.open(); new Thread(() -&gt; &#123; try &#123; // 对应IO编程中服务端启动 ServerSocketChannel listenerChannel = ServerSocketChannel.open(); listenerChannel.socket().bind(new InetSocketAddress(3333)); listenerChannel.configureBlocking(false); // 配置NIO非阻塞模式 listenerChannel.register(serverSelector, SelectionKey.OP_ACCEPT); while (true) &#123; // 监测是否有新的连接，这里的1指的是阻塞的时间为 1ms if (serverSelector.select(1) &gt; 0) &#123; Set&lt;SelectionKey&gt; set = serverSelector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = set.iterator(); while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isAcceptable()) &#123; try &#123; // 每来一个新连接，不需要创建一个线程，而是直接注册到clientSelector SocketChannel clientChannel = ((ServerSocketChannel) key.channel()).accept(); clientChannel.configureBlocking(false); clientChannel.register(clientSelector, SelectionKey.OP_READ); &#125; finally &#123; keyIterator.remove(); &#125; &#125; &#125; &#125; &#125; &#125; catch (IOException ignored) &#123; &#125; &#125; ).start(); new Thread(() -&gt; &#123; try &#123; while (true) &#123; // (2) 批量轮询是否有哪些连接有数据可读，这里的1指的是阻塞的时间为 1ms if (clientSelector.select(1) &gt; 0) &#123; Set&lt;SelectionKey&gt; set = clientSelector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = set.iterator(); while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isReadable()) &#123; try &#123; SocketChannel clientChannel = (SocketChannel) key.channel(); ByteBuffer byteBuffer = ByteBuffer.allocate(1024); // (3) 面向 Buffer clientChannel.read(byteBuffer); byteBuffer.flip(); System.out.println(Charset.defaultCharset().newDecoder().decode(byteBuffer).toString()); &#125; finally &#123; keyIterator.remove(); key.interestOps(SelectionKey.OP_READ); &#125; &#125; &#125; &#125; &#125; &#125; catch (IOException ignored) &#123; &#125; &#125; ).start(); &#125;&#125; 总结NIO可让您只使用一个（或几个）单线程管理多个通道（网络连接或文件），但付出的代价是解析数据可能会比从一个阻塞流中读取数据更复杂。 如果需要管理同时打开的成千上万个连接，这些连接每次只是发送少量的数据，例如聊天服务器，实现NIO的服务器可能是一个优势。同样，如果你需要维持许多打开的连接到其他计算机上，如P2P网络中，使用一个单独的线程来管理你所有出站连接，可能是一个优势。 优点: 减少线程，减少系统线程创建、线程切换的开销缺点: 循环中造成系统资源浪费","categories":[{"name":"Java - NIO","slug":"Java-NIO","permalink":"http://blog.luodexin.com/categories/Java-NIO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.luodexin.com/tags/Java/"},{"name":"NIO","slug":"NIO","permalink":"http://blog.luodexin.com/tags/NIO/"}]},{"title":"Java并发编程-Future","slug":"java/02.advenced-features/03.Concurrent","date":"2017-12-10T12:17:59.000Z","updated":"2020-11-08T00:25:08.288Z","comments":false,"path":"posts/2f8bc63b.html","link":"","permalink":"http://blog.luodexin.com/posts/2f8bc63b.html","excerpt":"","text":"1. Future 和 FutureTask1.1 Future 接口Future 接口代表一个异步计算的结果。Future 接口提供了检查计算是否完成、等待计算完成、取回计算结果等方法。 1234567891011121314// 取消一个任务，并返回取消结果。参数表示是否中断线程。boolean cancel(boolean mayInterruptInRunning);// 判断任务是否被取消boolean isCancelled(); // 判断当前任务是否执行完毕，包括正常执行完毕、执行异常或者任务取消。Boolean isDone();// 获取任务执行结果，任务结束之前会阻塞。V get();// 在指定时间内尝试获取执行结果。若超时则抛出超时异常V get(long timeout, TimeUnit unit) 1.1 FutureTask 类FutureTask 是 Future 的一个基本实现，它代表了一个可取消的异步计算。 它提供了检查计算是否完成、等待计算完成、取回计算结果等方法。取回计算结果方法在调用时会阻塞线程，直到计算结束。当一个计算已经完成后，不能重新开始或者取消。 类继承关系： FutureTask实现了RunnableFuture接口，而RunnableFuture继承了Runnable和Future，也就是说FutureTask既是Runnable，也是Future。 核心属性 volatile int state表示对象状态，volatile关键字保证了内存可见性。futureTask中定义了7种状态，代表了7种不同的执行状态。 1234567private static final int NEW = 0; //任务新建和执行中private static final int COMPLETING = 1; //任务将要执行完毕private static final int NORMAL = 2; //任务正常执行结束private static final int EXCEPTIONAL = 3; //任务异常private static final int CANCELLED = 4; //任务取消private static final int INTERRUPTING = 5; //任务线程即将被中断private static final int INTERRUPTED = 6; //任务线程已中断 Callable callable：被提交的任务 Object outcome：任务执行结果或者任务异常 volatile Thread runner：执行任务的线程 volatile WaitNode waiters：等待节点，关联等待线程 long stateOffset：state字段的内存偏移量 long runnerOffset：runner字段的内存偏移量 long waitersOffset：waiters字段的内存偏移量 内部状态转换FutureTask中使用state表示任务状态，state值变更的由CAS操作保证原子性。FutureTask对象初始化时，在构造器中把state设置为NEW，之后状态的变更依据具体执行情况来定。 例如任务执行正常结束前，state会被设置成COMPLETING，代表任务即将完成，接下来很快就会被设置为NORMAL或者EXCEPTIONAL，这取决于调用Runnable中的call()方法是否抛出了异常。有异常则后者，反之前者。 任务提交后、任务结束前取消任务，那么有可能变为CANCELLED或者INTERRUPTED。在调用cancel方法时，如果传入false表示不中断线程，state会被置为CANCELLED，反之state先被变为INTERRUPTING，后变为INTERRUPTED。 FutureTask的状态流转过程，可以出现以下四种情况： 任务正常执行并返回。 NEW -&gt; COMPLETING -&gt; NORMAL 执行中出现异常。NEW -&gt; COMPLETING -&gt; EXCEPTIONAL 任务执行过程中被取消，并且不响应中断。NEW -&gt; CANCELLED 任务执行过程中被取消，并且响应中断。 NEW -&gt; INTERRUPTING -&gt; INTERRUPTED 执行过程 1234567891011121314151617181920212223242526272829303132333435public void run() &#123; // 校验任务状态 校验当前任务状态是否为NEW以及runner是否已赋值。这一步是防止任务被取消。 if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; try &#123; Callable&lt;V&gt; c = callable; if (c != null &amp;&amp; state == NEW) &#123; V result; boolean ran; try &#123; // 执行业务逻辑，也就是c.call()方法被执行 result = c.call(); ran = true; &#125; catch (Throwable ex) &#123; result = null; ran = false; // 如果业务逻辑异常，则调用setException方法将异常对象赋给outcome，并且更新state值 setException(ex); &#125; if (ran) // 如果业务正常，则调用set方法将执行结果赋给outcome，并且更新state值 set(result); &#125; &#125; finally &#123; // 重置runner // runner must be non-null until state is settled to prevent concurrent calls to run() runner = null; // state must be re-read after nulling runner to prevent leaked interrupts int s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125;&#125; 这个方法经历了以下几步: 校验当前任务状态是否为NEW以及runner是否已赋值。这一步是防止任务被取消。 double-check任务状态state 执行业务逻辑，也就是c.call()方法被执行 如果业务逻辑异常，则调用setException方法将异常对象赋给outcome，并且更新state值 如果业务正常，则调用set方法将执行结果赋给outcome，并且更新state值 2. CountDownLatchCountDown表示减法计数，Latch表示门闩的意思，计数为0的时候就可以打开门闩了。CountDownLatch 使用时指定一个计数初始值(通常是子线程数量)， 在子线程中执行完时 finnaly 代码中使用 countDownLatch.countDown()。 主线程(或调用线程)中使用 countDownLatch.await() 方法阻塞，等到所有子线程的将 countDownLatch 计数器减为0时继续执行。 3. CyclicBarrierCyclicBarrier 是一个加计数器。它表示一个同步栅栏，当线程执行到 cyclicBarrier.await() 的地方时会阻塞。当加计数器加到指定的数是，所有阻塞线程一起放行。 4. 使用 FutureTask 优化 RPC 调用12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * service1 和 service2 是两个RPC调用服务 * 使用 CountDownLatch 协调 */@Testpublic void testFutureTask5() throws InterruptedException &#123; /** 创建线程池， 正式环境使用全局的线程池，而且通常自定义的线程池, 拒绝策略改为 CallerRunsPolicy **/ ExecutorService threadPool = Executors.newFixedThreadPool(5); // 两个 RPC 调用， 创建一个 countDownLatch，初始值是2 CountDownLatch countDownLatch = new CountDownLatch(2); FutureTask&lt;Integer&gt; task1 = new FutureTask&lt;&gt;(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; Integer result = null; try &#123; result = service1.rpc(); &#125; finally &#123; countDownLatch.countDown(); &#125; return result; &#125; &#125;); FutureTask&lt;Integer&gt; task2 = new FutureTask&lt;&gt;(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; Integer result = null; try &#123; result = service2.rpc(); &#125; finally &#123; countDownLatch.countDown(); &#125; return result; &#125; &#125;); threadPool.submit(task1); threadPool.submit(task2); // 阻塞调用者线程，等待两个 RPC 调用完成 countDownLatch.await(); System.out.println(&quot;远程调用完成&quot;); // 所有RPC调用完成, 取出调用结果封装接口返回 Integer result1 = null, result2 = null; try &#123; result1 = task1.get(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; try &#123; result2 = task2.get(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;所有RPC 调用完成 task1: &quot; + result1 + &quot; task2: &quot; + result2);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * 使用 CyclicBarrier 协调线程 */@Testpublic void testFutureTask6() throws InterruptedException, BrokenBarrierException &#123; /** 创建线程池， 正式环境使用全局的线程池，而且通常自定义的线程池, 拒绝策略改为 CallerRunsPolicy **/ ExecutorService threadPool = Executors.newFixedThreadPool(5); // 创建一个栅栏，两个 RPC 和 caller 线程， 使用栅栏来同步 CyclicBarrier cyclicBarrier = new CyclicBarrier(3); FutureTask&lt;Integer&gt; task1 = new FutureTask&lt;&gt;(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; Integer result = null; try &#123; result = service1.rpc(); &#125; finally &#123; cyclicBarrier.await(); &#125; return result; &#125; &#125;); FutureTask&lt;Integer&gt; task2 = new FutureTask&lt;&gt;(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; Integer result = null; try &#123; result = service2.rpc(); &#125; finally &#123; cyclicBarrier.await(); &#125; return result; &#125; &#125;); threadPool.submit(task1); threadPool.submit(task2); // 栅栏同步点: call 线程在此等待 RPC 线程到达同步点 cyclicBarrier.await(); System.out.println(&quot;远程调用完成&quot;); // 所有RPC调用完成, 取出调用结果封装接口返回 Integer result1 = null, result2 = null; try &#123; result1 = task1.get(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; try &#123; result2 = task2.get(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;所有RPC 调用完成 task1: &quot; + result1 + &quot; task2: &quot; + result2);&#125;","categories":[{"name":"Java - Concurrent","slug":"Java-Concurrent","permalink":"http://blog.luodexin.com/categories/Java-Concurrent/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.luodexin.com/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"http://blog.luodexin.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"Java 日志系统","slug":"java/03.log-system/logger","date":"2017-12-10T12:17:59.000Z","updated":"2020-11-07T11:54:31.489Z","comments":false,"path":"posts/a7321843.html","link":"","permalink":"http://blog.luodexin.com/posts/a7321843.html","excerpt":"","text":"1.日志系统背景/发展史那就要从Java Log的发展历程开始说起。 log4j（作者Ceki Gülcü）出来时就等到了广泛的应用（注意这里是直接使用），是Java日志事实上的标准，并成为了Apache的项目因此，早年，你工作的时候，在日志里使用了log4j框架来输出，于是你代码是这么写的12345import org.apache.log4j.Logger;\\\\省略Logger logger = Logger.getLogger(Test.class);logger.trace(&quot;trace&quot;);\\\\省略 Apache要求把log4j并入到JDK，SUN拒绝，并在jdk1.4版本后增加了JUL （java.util.logging） 毕竟是JDK自带的，JUL也有很多人用。同时还有其他日志组件，如SimpleLog等。使用JUL，你可能这么来输出系统日志12345import java.util.logging.Logger；\\\\省略Logger loggger = Logger.getLogger(Test.class.getName()); logger.finest(&quot;finest&quot;);\\\\省略 这时如果有人想换成其他日志组件，如log4j换成JUL，因为api完全不同，就需要改动代码。 Apache见此，开发了JCL（Jakarta Commons Logging），即commons-logging-xx.jar。它只提供一套通用的日志接口api，并不提供日志的实现。很好的设计原则嘛，依赖抽象而非实现。这样应用程序可以在运行时选择自己想要的日志实现组件。 这样看上去也挺美好的，但是log4j的作者觉得JCL不好用，自己开发出slf4j，它跟JCL类似，本身不替供日志具体实现，只对外提供接口或门面。目的就是为了替代JCL。同时，还开发出logback，一个比log4j拥有更高性能的组件，目的是为了替代log4j。 Apache参考了logback,并做了一系列优化，推出了log4j2 2.使用 Apache commons-logging 日志接口commons-logging 已经停止更新，最后的状态如下所示： JCL默认的配置：如果能找到Log4j 则默认使用log4j 实现，如果没有则使用jul(jdk自带的) 实现，再没有则使用JCL内部提供的SimpleLog 实现。 至于这个Log具体的实现类，JCL会在ClassLoader中进行查找。这么做，有三个缺点，缺点一是效率较低，二是容易引发混乱，三是在使用了自定义ClassLoader的程序中，使用JCL会引发内存泄露。于是log4j的作者觉得JCL不好用，自己又写了一个新的接口api，那么就是slf4j。 3.使用日志门面slf4jslf4j 的集成图如下图所示 如图所示，应用调了sl4j-api，即日志门面接口。日志门面接口本身通常并没有实际的日志输出能力，它底层还是需要去调用具体的日志框架API的，也就是实际上它需要跟具体的日志框架结合使用。由于具体日志框架比较多，而且互相也大都不兼容，日志门面接口要想实现与任意日志框架结合可能需要对应的桥接器，上图红框中的组件即是对应的各种桥接器！ SLF4J与其它日志实现组件的结合使用 图的意思为如果你想用slf4j作为日志门面的话，你如何去配合使用其他日志实现组件，这里说明一下（注意jar包名缺少了版本号，在找版本时也要注意版本之间是否兼容） slf4j + logbackslf4j-api.jar + logback-classic.jar + logback-core.jar slf4j + log4jslf4j-api.jar + slf4j-log4j12.jar + log4j.jar slf4j + julslf4j-api.jar + slf4j-jdk14.jar 也可以只用slf4j无日志实现slf4j-api.jar + slf4j-nop.jar SLF4J与其它日志接口适配slf4j支持各种适配，无论你现在是用哪种日志组件，你都可以通过slf4j的适配器来使用上slf4j。只要你切换到了slf4j， 那么再通过slf4j用上实现组件，即上面说的。 其实总的来说，无论就是以下几种情况 你在用JCL使用jcl-over-slf4j.jar适配 你在用log4j使用log4j-over-slf4j.jar适配 你在用JUL使用jul-to-slf4j.jar适配 jcl-over-slf4j.jar 与 slf4j-jcl.jar当系统是面向commons-logging开发，但是希望使用slf4j来作为日志接口，可以使用jcl-over-slf4j.jar来适配，将commons-logging的日志请求重定向到slf4j。 当应用面向slf4j开发后，出于某些原因需要切换回commons-logging，slf4j允许你后悔，可以通过slf4j-jcl.jar适配将所有slf4j的日志请求重定向到commons-logging。 icl-over-slf4j.jar 与 slf4j-jcl.jar两者不能同时使用，否则会出现日志委托陷入死循环。 4.常见案例案例一：项目中一个模块用log4j，另一个模块用slf4j+log4j2,统一输出其实在某些中小型公司，这种情况很常见。我曾经见过某公司的项目，因为研发不懂底层的日志原理，日志文件里头既有log4j.properties，又有log4j2.xml，各种API混用，惨不忍睹!还有人用着jul的API，然后拿着log4j.properties，跑来问我，为什么配置不生效！简直是一言难尽！OK，回到我们的问题，如何统一输出！OK，这里就要用上slf4j的适配器，slf4j提供了各种各样的适配器，用来将某种日志框架委托给slf4j。其最明显的集成工作方式有如下: 进行选择填空，将我们的案例里的条件填入,显然应该选log4j-over-slf4j适配器，就变成下面这张图 案例二：如何让Spring统一日志输出统一日志输出的好处就是我们可以统一项目内的各个模块、框架的日志输出（日志格式，日志文件，存放路径等，以及其他slf4j支持的功能）。Spring是用JCL作为日志门面的，当我们的应用是slf4j + log4j，怎么让Spring也用到log4j作为日志输出来实现统一日志输出呢？ spring默认使用的是jcl输出日志，由于你此时并没有引入Log4j的日志框架，jcl会以jul做为日志框架。此时集成图如下 而你的应用中，采用了slf4j+log4j-core，即log4j2进行日志记录，那么此时集成图如下 适配思路 首先确认需要统一日志的模块、框架是使用哪个日志组件的，然后再找到sfl4j的适配器。 记得去掉无用的日志实现组件，只保留你要用的。 方案一 使用jcl-over-slf4j适配器，将spring内部的commons-logging 桥接到我们的应用到slf4j日志门面，使用我们的应用中的日志系统，去掉spring中的引入的日志组件，此时集成图就变成下面这样了 方案二 使用jul-to-slf4j适配器，将spring中的JUL日志组件的日志输出桥接到我们的应用的slf4j日志门面，此时集成图如下所示： 注意：这种方案中需要在程序中执行如下代码 12SLF4JBridgeHandler.removeHandlersForRootLogger();SLF4JBridgeHandler.install();","categories":[{"name":"Java - Concurrent","slug":"Java-Concurrent","permalink":"http://blog.luodexin.com/categories/Java-Concurrent/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.luodexin.com/tags/Java/"},{"name":"日志","slug":"日志","permalink":"http://blog.luodexin.com/tags/%E6%97%A5%E5%BF%97/"}]},{"title":"Java线程池二","slug":"java/02.advenced-features/02.ThreadPool-02","date":"2017-12-08T02:45:13.000Z","updated":"2020-11-08T00:25:20.927Z","comments":false,"path":"posts/ab5fe11c.html","link":"","permalink":"http://blog.luodexin.com/posts/ab5fe11c.html","excerpt":"","text":"1. 为什么要使用线程池？创建线程和销毁线程的花销是比较大的，这些时间有可能比处理业务的时间还要长。这样频繁的创建线程和销毁线程，再加上业务工作线程，消耗系统资源的时间，可能导致系统资源不足。（我们可以把创建和销毁的线程的过程去掉） 记创建线程消耗时间 T1，执行任务消耗时间 T2，销毁线程消耗时间 T3 如果 T1+T3&gt;T2，那么是不是说开启一个线程来执行这个任务太不划算了！ 正好，线程池缓存线程，可用已有的闲置线程来执行新任务，避免了 T1+T3 带来的系统开销 2. 线程池有什么作用？ 提高效率: 创建好一定数量的线程放在池中，等需要使用的时候就从池中拿一个，这要比需要的时候创建一个线程对象要快的多。避免了创建线程、销毁线程的系统开销。 控制并发量: 程能共享系统资源，如果同时执行的线程过多，就有可能导致系统资源不足而产生阻塞的情况。运用线程池能有效的控制线程最大并发数，避免以上的问题。 方便管理可以编写线程池管理代码对池中的线程同一进行管理，比如说启动时有该程序创建100个线程，每当有请求的时候，就分配一个线程去工作，如果刚好并发有101个请求，那多出的这一个请求可以排队等候，避免因无休止的创建线程导致系统崩溃。 3. 说说几种常见的线程池及使用场景 newSingleThreadExecutor创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 newFixedThreadPool创建一个固定大小线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行 4. 为什么不推荐使用 Executors 创建线程池？Executors各个方法的弊端？线程池不允许使用Executors去创建，而是通过ThreadPoolExecutor的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。 弊端: newSingleThreadExecutor 和 newFixedThreadPool主要问题是堆积的请求处理队列可能会耗费非常大的内存，甚至OOM。 newCachedThreadPool和newScheduledThreadPool主要问题是最大线程数是 Integer.MAX_VALUE，可能会创建数量非常多的线程，甚至OOM。 5. 线程池都有哪几种工作队列 ArrayBlockingQueue是一个基于数组结构的有界阻塞队列，此队列按 FIFO（先进先出）原则对元素进行排序。 LinkedBlockingQueue一个基于链表结构的阻塞队列，此队列按FIFO （先进先出） 排序元素，吞吐量通常要高于ArrayBlockingQueue。静态工厂方法 Executors.newFixedThreadPool() 使用了这个队列。 SynchronousQueue一个不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于LinkedBlockingQueue，静态工厂方法Executors.newCachedThreadPool使用了这个队列。 PriorityBlockingQueue一个具有优先级的无限阻塞队列。 5. 线程池中的几种重要的参数及流程说明123456789101112131415161718192021public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; &#125; corePoolSize：核心池的大小，这个参数跟后面讲述的线程池的实现原理有非常大的关系。在创建了线程池后，默认情况下，线程池中并没有任何线程，而是等待有任务到来才创建线程去执行任务，除非调用了prestartAllCoreThreads() 或者 prestartCoreThread() 方法，从这2个方法的名字就可以看出，是预创建线程的意思，即在没有任务到来之前就创建corePoolSize个线程或者一个线程。默认情况下，在创建了线程池后，线程池中的线程数为0，当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到corePoolSize后，就会把到达的任务放到缓存队列当中； maximumPoolSize：线程池最大线程数，这个参数也是一个非常重要的参数，它表示在线程池中最多能创建多少个线程； keepAliveTime：表示线程没有任务执行时最多保持多久时间会终止。默认情况下，只有当线程池中的线程数大于corePoolSize时，keepAliveTime才会起作用，直到线程池中的线程数不大于corePoolSize，即当线程池中的线程数大于corePoolSize时，如果一个线程空闲的时间达到keepAliveTime，则会终止，直到线程池中的线程数不超过corePoolSize。但是如果调用了allowCoreThreadTimeOut(boolean)方法，在线程池中的线程数不大于corePoolSize时，keepAliveTime参数也会起作用，直到线程池中的线程数为0； unit：参数keepAliveTime的时间单位，有7种取值 workQueue：一个阻塞队列，用来存储等待执行的任务，这个参数的选择也很重要，会对线程池的运行过程产生重大影响 ArrayBlockingQueue和PriorityBlockingQueue使用较少，一般使用LinkedBlockingQueue和SynchronousQueue。线程池的排队策略与BlockingQueue有关。 threadFactory：用于设置创建线程的工厂，可以通过线程工厂给每个创建出来的线程做些更有意义的事情，比如设置daemon和优先级等等 handler: 表示当拒绝处理任务时的策略，有以下四种取值 AbortPolicy：直接抛出异常。 CallerRunsPolicy：使用用调用者所在线程来运行任务。 DiscardOldestPolicy：丢弃队列里最近的一个任务，并执行当前任务。 DiscardPolicy：不处理，丢弃掉。 也可以根据应用场景需要来实现 RejectedExecutionHandler 接口自定义策略。如记录日志或持久化不能处理的任务。 6. 怎么理解无界队列和有界队列 有界队列 初始的 poolSize &lt; corePoolSize，提交的runnable任务，会直接做为new一个Thread的参数，立马执行 。 当提交的任务数超过了corePoolSize，会将当前的runable提交到一个block queue中。 有界队列满了之后，如果 poolSize &lt; maximumPoolsize 时，会尝试new 一个Thread的进行救急处理，立马执行对应的runnable任务。 如果3中也无法处理了，就会走到第四步执行reject操作。 无界队列与有界队列相比，除非系统资源耗尽，否则无界的任务队列不存在任务入队失败的情况。当有新的任务到来，系统的线程数小于corePoolSize时，则新建线程执行任务。当达到corePoolSize后，就不会继续增加，若后续仍有新的任务加入，而没有空闲的线程资源，则任务直接进入队列等待。若任务创建和处理的速度差异很大，无界队列会保持快速增长，直到耗尽系统内存。 当线程池的任务缓存队列已满并且线程池中的线程数目达到maximumPoolSize，如果还有任务到来就会采取任务拒绝策略。 7. CountDownLatch 和 CyclicBarrierCountDownLatch: CountDown表示减法计数，Latch表示门闩的意思，计数为0的时候就可以打开门闩了。CyclicBarrier: Cyclic Barrier表示循环的障碍物。其实是一个加计数器，当加到指定值再进行下一步操作。","categories":[{"name":"Java - Concurrent","slug":"Java-Concurrent","permalink":"http://blog.luodexin.com/categories/Java-Concurrent/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.luodexin.com/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"http://blog.luodexin.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"Java Map","slug":"java/01.language-foundation/map","date":"2017-12-07T12:17:59.000Z","updated":"2020-11-07T11:52:22.085Z","comments":false,"path":"posts/7d046c4b.html","link":"","permalink":"http://blog.luodexin.com/posts/7d046c4b.html","excerpt":"","text":"Hash 存储中冲突处理方法 开发定址法 再hash法 链地址法(拉链法) 建立公共溢出区 1. HashMap1.1 HashMap由数组和链表来实现对数据的存储HashMap采用Entry数组来存储key-value对，每一个键值对组成了一个Entry实体，Entry类实际上是一个单向的链表结构，它具有Next指针，可以连接下一个Entry实体，以此来解决Hash冲突的问题。 JDK8之后，如果哈希表单向链表中元素超过8个，那么单向链表这种数据结构会变成红黑树数据结构。当红黑树上的节点数量小于6个，会重新把红黑树变成单向链表数据结构。 1.2 HashMap 的特性 数组初始长度16，数组长度是2的n次方，目的是为了减少碰撞这个哈希算法实际就是取模，hash%length，计算机中直接求余效率不如位移运算，源码中做了优化hash&amp;(length-1)，hash%length == hash&amp;(length-1) 的前提是 length 是2的n次方； 默认填充因子 0.75,在理想情况下,使用随机哈希码,节点出现的频率在hash桶中遵循泊松分布。也就是说这个默认加载因子是当hashMap集合底层数组的容量达到75%时，数组就开始扩容。每次扩容为原来的2倍。 线程不安全 key可以为null, value 可以为null, 最多放一个key为null的键值对 无序： 因为不一定挂到哪一个单向链表上的，因此加入顺序和取出也不一样。 不可重复: 使用equals方法来保证HashMap集合key不可重复，如key重复来，value就会覆盖。 HashMap中的“死锁”: 扩容的时候线程不安全造成的 其迭代器是fail-fast的 如果有其它线程对HashMap进行的添加/删除元素，将会抛出ConcurrentModificationException，但迭代器本身的remove方法移除元素则不会抛出异常。这条同样也是Enumeration和Iterator的区别。 2. LinkedHashMapLinkedHashMap 继承自 HashMap，在 HashMap 基础上，通过维护一条双向链表，解决了 HashMap 不能随时保持遍历顺序和插入顺序一致的问题。除此之外，LinkedHashMap 对访问顺序也提供了相关支持。在一些场景下，该特性很有用，比如缓存。在实现上，LinkedHashMap 很多方法直接继承自 HashMap，仅为维护双向链表覆写了部分方法。 上图中，淡蓝色的箭头表示前驱引用，红色箭头表示后继引用。每当有新键值对节点插入，新节点最终会接在 tail 引用指向的节点后面。而 tail 引用则会移动到新的节点上，这样一个双向链表就建立起来了。 2.2 特性 继承自HashMap, 底层存储基本相同，增加了双链表 key可以为null, value 可以为null, 最多放一个key为null的键值对 3. TreeMap3.2 TreeMap 特性 是一个有序的key-value集合，底层通过红黑树实现红黑树的性质: 每个节点都只能是红色或者黑色 根节点是黑色 每个叶节点（NIL节点，空节点）是黑色的 如果一个结点是红的，则它两个子节点都是黑的。也就是说在一条路径上不能出现相邻的两个红色结点 从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点这些约束强制了红黑树的关键性质: 从根到叶子的最长的可能路径不多于最短的可能路径的两倍长。结果是这棵树大致上是平衡的。 红黑树的基本操作: 左旋、右旋、着色。 TreeMap 中 key 不能为 null, Value 可以为 null 实现了NavigableMap接口，意味着它支持一系列的导航方法。比如返回有序的key集合 TreeMap 是有序的，该映射根据其键的自然顺序(字母排序)进行排序，或者根据创建映射时提供的 Comparator 进行排序，具体取决于使用的构造方法。 TreeMap 非线程安全的。 它的iterator 方法返回的迭代器是fail-fast的 4. HashTableHashTable类中，保存实际数据的，依然是Entry对象。其数据结构与HashMap是相同的(使用数组加链表，不会转换成红黑树)。 4.1 特性 使用数组链表存储 是线程安全的,效率低 Hashtable不可以存储null键和null值 (put方法中，value为空之间抛出异常，key为空在调用key.hashCode()的时候异常) HashTable创建的时候如果不指定容量大小，初始容量大小为11，之后每次扩充，容量会变为2n + 1；","categories":[{"name":"Java - Foundation","slug":"Java-Foundation","permalink":"http://blog.luodexin.com/categories/Java-Foundation/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.luodexin.com/tags/Java/"}]},{"title":"Java线程池一","slug":"java/02.advenced-features/01.ThreadPool-01","date":"2017-12-07T12:17:59.000Z","updated":"2020-11-07T11:52:28.521Z","comments":false,"path":"posts/90dfcfb5.html","link":"","permalink":"http://blog.luodexin.com/posts/90dfcfb5.html","excerpt":"","text":"1. Overview1.1 并发工具包(Concurrency utilities)介绍并发工具包提供了： 高性能、灵活的线程池实现 异步任务执行框架 一组为并发访问优化了的集合类 同步工具如计数器、原子变量、锁、条件变量等 1.2 核心类 Interface： Executor: 用于执行提交的任务，将任务的提交与任务的调度执行隔离开 ExecutorService： Abstract Class: AbstractExecutorService Implimention ThreadPoolExecutor ScheduledThreadPoolExecutor ExecutorCompletionService 2.核心类详情2.1 Executor接口Executor是一个顶层接口，在它里面只声明了一个方法execute(Runnable)，返回值为void，参数为Runnable类型，从字面意思可以理解，就是用来执行传进去的任务的。该接口只有一个方法 void execute(Runnable command); ，该方法用于在将来执行一个提交的任务，该任务可能在新的线程执行、在线程池执行、在调用者线程执行，具体执行方式由实现类实现。 在调用者线程执行 12345class DirectExecutor implements Executor &#123; public void execute(Runnable r) &#123; r.run(); &#125;&#125; 在新线程执行 12345class ThreadPerTaskExecutor implements Executor &#123; public void execute(Runnable r) &#123; new Thread(r).start(); &#125;&#125; 序列化执行 12345678910111213141516171819202122232425262728293031 class SerialExecutor implements Executor &#123; final Queue&lt;Runnable&gt; tasks = new ArrayDeque&lt;Runnable&gt;(); final Executor executor; Runnable active; SerialExecutor(Executor executor) &#123; this.executor = executor; &#125; public synchronized void execute(final Runnable r) &#123; tasks.offer(new Runnable() &#123; public void run() &#123; try &#123; r.run(); &#125; finally &#123; scheduleNext(); &#125; &#125; &#125;); if (active == null) &#123; scheduleNext(); &#125; &#125; protected synchronized void scheduleNext() &#123; if ((active = tasks.poll()) != null) &#123; executor.execute(active); &#125; &#125;&#125; 2.2 ExecutorService 接口ExecutorService接口继承了Executor接口，并声明了一些管理线程池的方法：submit、invokeAll、invokeAny 以及 shutDown 等 2.3 AbstractExecutorServiceAbstractExecutorService 是一个抽象类，实现了ExecutorService接口，基本实现了ExecutorService中声明的所有方法； 3. 线程池 ThreadPoolExecutor 分析4. 常用线程池Executors 类中提供了一些静态方法，能够非常方便地创建一些常用的线程池。 newSingleThreadExecutor创建一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。 newFixedThreadPool(n)创建固定大小的线程池。每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。 newCachedThreadPool (推荐使用)创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲（60秒不执行任务）的线程，当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对线程池大小做限制，线程池大小完全依赖于操作系统（或者说JVM）能够创建的最大线程大小。 newScheduledThreadPool大小无限制的线程池，支持定时和周期性的执行线程","categories":[{"name":"Java - Concurrent","slug":"Java-Concurrent","permalink":"http://blog.luodexin.com/categories/Java-Concurrent/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.luodexin.com/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"http://blog.luodexin.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]}],"categories":[{"name":"MQ - Kafka","slug":"MQ-Kafka","permalink":"http://blog.luodexin.com/categories/MQ-Kafka/"},{"name":"Spring - SpringCloud","slug":"Spring-SpringCloud","permalink":"http://blog.luodexin.com/categories/Spring-SpringCloud/"},{"name":"Spring - SpringFramework","slug":"Spring-SpringFramework","permalink":"http://blog.luodexin.com/categories/Spring-SpringFramework/"},{"name":"Redis - 常见问题","slug":"Redis-常见问题","permalink":"http://blog.luodexin.com/categories/Redis-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.luodexin.com/categories/MQ/"},{"name":"Java - JVM","slug":"Java-JVM","permalink":"http://blog.luodexin.com/categories/Java-JVM/"},{"name":"Linux - 常用命令","slug":"Linux-常用命令","permalink":"http://blog.luodexin.com/categories/Linux-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"name":"Java - NIO","slug":"Java-NIO","permalink":"http://blog.luodexin.com/categories/Java-NIO/"},{"name":"Java - Concurrent","slug":"Java-Concurrent","permalink":"http://blog.luodexin.com/categories/Java-Concurrent/"},{"name":"Java - Foundation","slug":"Java-Foundation","permalink":"http://blog.luodexin.com/categories/Java-Foundation/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://blog.luodexin.com/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"http://blog.luodexin.com/tags/Kafka/"},{"name":"面试","slug":"面试","permalink":"http://blog.luodexin.com/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Spring","slug":"Spring","permalink":"http://blog.luodexin.com/tags/Spring/"},{"name":"Zuul","slug":"Zuul","permalink":"http://blog.luodexin.com/tags/Zuul/"},{"name":"AOP","slug":"AOP","permalink":"http://blog.luodexin.com/tags/AOP/"},{"name":"源码","slug":"源码","permalink":"http://blog.luodexin.com/tags/%E6%BA%90%E7%A0%81/"},{"name":"Redis","slug":"Redis","permalink":"http://blog.luodexin.com/tags/Redis/"},{"name":"Java","slug":"Java","permalink":"http://blog.luodexin.com/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"http://blog.luodexin.com/tags/JVM/"},{"name":"Linux","slug":"Linux","permalink":"http://blog.luodexin.com/tags/Linux/"},{"name":"NIO","slug":"NIO","permalink":"http://blog.luodexin.com/tags/NIO/"},{"name":"多线程","slug":"多线程","permalink":"http://blog.luodexin.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"日志","slug":"日志","permalink":"http://blog.luodexin.com/tags/%E6%97%A5%E5%BF%97/"}]}